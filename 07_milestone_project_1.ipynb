{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone Project 1: Food Vision\n",
    "\n",
    "* Use TensorFlow datasets to download and explore data\n",
    "* Create a preprocessing function for the data\n",
    "* Batch and prepare datasets for modelling\n",
    "* Set up mixed precision training\n",
    "\n",
    "Food101\n",
    "    Training: 75,750 images (750 per class)\n",
    "    Testing: 250 images per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "# Need compute capability score of 7.0 or higher for mixed precision training\n",
    "# Mine = GeForce RTX 3070 Notebook GPU \n",
    "#   Compute capability: 8.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get custom images\n",
    "!wget -nc -P ../Downloads/ https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import series of helper functions for the notebook\n",
    "import sys\n",
    "# append the downloads path\n",
    "sys.path.append(\"../Downloads\")\n",
    "from _helper_functions import create_tensorboard_callback, plot_loss_curves, compare_historys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use TensorFlow datasets to download and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFDS - Tensorflow Datasets\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List datasets\n",
    "datasets_list = tfds.list_builders()\n",
    "print(\"food101\" in datasets_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check download size (some datasets are HUGE!)\n",
    "food_101_info = tfds.builder(name='food101').info\n",
    "print(food_101_info.download_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, test_data), ds_info = tfds.load(name=\"food101\",\n",
    "                                            data_dir=\"../Downloads\",\n",
    "                                            split=[\"train\", \"validation\"],\n",
    "                                            shuffle_files=True,\n",
    "                                            as_supervised=True, # data returned as tuple (data, label)\n",
    "                                            with_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Food101 Data from TensorFlow Datasets\n",
    "\n",
    "* Class names\n",
    "* Shape of input data (image tensors)\n",
    "* Datatype of input data\n",
    "* One-hot encoded or label encoded\n",
    "* Do the labels match up with the class names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features of Food101 from TFDS\n",
    "print(ds_info.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class names\n",
    "class_names = ds_info.features[\"label\"].names\n",
    "class_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take one sample from the train dataset\n",
    "\n",
    "train_one_sample = train_data.take(1) # samples are in format (image_tensor, label)\n",
    "train_one_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output info about the training sample\n",
    "for image, label in train_one_sample:\n",
    "    print(f\"\"\"\n",
    "    Image shape: {image.shape}\n",
    "    Image datatype: {image.dtype}\n",
    "    Target class from Food101 (tensor form): {label}\n",
    "    Class name (str form): {class_names[label.numpy()]}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the image tensor from TFDS's Food101 look like?\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check min and max values of the image tensor\n",
    "tf.reduce_min(image), tf.reduce_max(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot image tensor\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image)\n",
    "plt.title(class_names[label.numpy()])\n",
    "plt.axis(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Preprocessing Functions for the Data\n",
    "\n",
    "Neural networks perform best when data is:\n",
    "* Batched\n",
    "* Normalized\n",
    "* etc.\n",
    "\n",
    "The Data:\n",
    "* Datatype - `uint8` dtype\n",
    "* Shape  - different sized images\n",
    "* Not Scaled (pixel values are betwween 0-255)\n",
    "\n",
    "What we need:\n",
    "* Data in `float32` dtype\n",
    "* For batches, TensorFlow likes all the tensors within a batch to be of the same size.\n",
    "* Scaled (normalized) values between 0-1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(image, label, img_shape=224, normalize=False):\n",
    "    \"\"\"\n",
    "    Converts image datatype to `float32` and reshapes image to [img_shape, img_shape, color_channels]\n",
    "    \"\"\"\n",
    "    image = tf.image.resize(image, [img_shape, img_shape])\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    if normalize:\n",
    "        image = image / 255.0\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess a single sample image and check outputs\n",
    "preprocessed_img = preprocess_img(image, label, normalize=False)[0]\n",
    "print(f\"Image before preprocessing:\\n {image[:2]}...,\\nShape:{image.shape},\\nDatatype:{image.dtype}\")\n",
    "print(f\"Image after prerpocessing:\\n {preprocessed_img[:2]}...,\\nShape:{preprocessed_img.shape},\\nDatatype:{preprocessed_img.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch and Prepare Datasets for Modeling\n",
    "\n",
    "Make data input pipeline run really fast.\n",
    "\n",
    "Reading: https://www.tensorflow.org/guide/data\n",
    "\n",
    "`image_dataset_from_directory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map preprocessing function to training (parallelize)\n",
    "train_data = train_data.map(map_func=preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# Shuffle train_data, turn it into batches and prefetch it (load it faster)\n",
    "train_data = train_data.shuffle(buffer_size=1024).batch(batch_size=128).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Map preprocessing function to test data\n",
    "test_data = test_data.map(map_func=preprocess_img, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size=128).prefetch(buffer_size=tf.data.AUTOTUNE).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakdown\n",
    "\n",
    "1. Map the preprocessing function (`preprocess_image`) across the training dataset\n",
    "2. Shuffle the dataset using buffer size of 1024\n",
    "3. Batch the dataset using a batch size of 32\n",
    "4. Prefetch the next batch (using `tf.data.experimental.AUTOTUNE`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Modelling Callbacks\n",
    "\n",
    "* TensorBoard callback to log training results.\n",
    "* ModelCheckpoint callback to save our model's progress after feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensorboard callback (import from _helper_functions.py)\n",
    "from _helper_functions import create_tensorboard_callback\n",
    "\n",
    "# Create ModelCheckpoint callback to save a model's progress after each epoch\n",
    "checkpoint_path = \"../checkpoints/food_101_milestone/milestone_1.ckpt\"\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                    monitor=\"val_accuracy\", \n",
    "                                                    save_best_only=True,                                                     \n",
    "                                                    save_weights_only=True,\n",
    "                                                    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Mixed Precision Training\n",
    "\n",
    "[TensorFlow Mixed Precision](https://www.tensorflow.org/guide/mixed_precision)\n",
    "\n",
    "Mixed precision is the use of both 16-bit and 32-bit floating-point types in a model during training to make it run faster and use less memory. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn on mixed precision training\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "mixed_precision.global_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Feature Extraction Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "mixed_precision.global_policy()\n",
    "# Create base model\n",
    "input_shape = (224, 224, 3)\n",
    "base_model = tf.keras.applications.EfficientNetV2B0(include_top=False)\n",
    "base_model.trainable = False # freeze base model layers\n",
    "\n",
    "# Create Functional model \n",
    "inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n",
    "# Note: EfficientNetBX models have rescaling built-in but if your model didn't you could have a layer like below\n",
    "# x = preprocessing.Rescaling(1./255)(x)\n",
    "x = base_model(inputs, training=False) # set base_model to inference mode only\n",
    "x = layers.GlobalAveragePooling2D(name=\"pooling_layer\")(x)\n",
    "x = layers.Dense(len(class_names))(x) # want one output neuron per class \n",
    "# Separate activation of output layer so we can output float32 activations\n",
    "outputs = layers.Activation(\"softmax\", dtype=tf.float32, name=\"softmax_float32\")(x) \n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", # Use sparse_categorical_crossentropy when labels are *not* one-hot\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check base model layers\n",
    "for layer in base_model.layers:\n",
    "    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the feature extraction model\n",
    "\n",
    "1. build a feature extraction model (train a couple output layers with base layers frozen)\n",
    "2. Fine-tune some of the frozen layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_101_food_classes_feature_extract = model.fit(\n",
    "    train_data, \n",
    "    epochs=3, \n",
    "    steps_per_epoch=len(train_data),\n",
    "    validation_data=test_data, \n",
    "    validation_steps=int(0.15 * len(test_data)),\n",
    "    callbacks=[\n",
    "        create_tensorboard_callback(\n",
    "            dir_name=\"../training_logs\", \n",
    "            experiment_name=\"efficientnetv2b0_101_classes_all_data_feature_extract\"\n",
    "        ),\n",
    "        model_checkpoint\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on whole dataset\n",
    "results_feature_extract_model = model.evaluate(test_data)\n",
    "results_feature_extract_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune\n",
    "\n",
    "Fine tune the model to beat the top-1 accuracy of 77.4%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Evaluate Checkpoint Weights\n",
    "We can load in and evaluate our model's checkpoints by:\n",
    "\n",
    "1. Cloning our model using `tf.keras.models.clone_model()` to make a copy of our feature extraction model with reset weights.\n",
    "2. Calling the `load_weights()` method on our cloned model passing it the path to where our checkpointed weights are stored.\n",
    "3. Calling `evaluate()` on the cloned model with loaded weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the model we created (this resets all weights)\n",
    "cloned_model = tf.keras.models.clone_model(model)\n",
    "cloned_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where are our checkpoints stored?\n",
    "checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpointed weights into cloned_model\n",
    "cloned_model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile cloned_model (with same parameters as original model)\n",
    "cloned_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                     optimizer=tf.keras.optimizers.Adam(),\n",
    "                     metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalaute cloned model with loaded weights (should be same score as trained model)\n",
    "results_cloned_model_with_loaded_weights = cloned_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaded checkpoint weights should return very similar results to checkpoint weights prior to saving\n",
    "import numpy as np\n",
    "# check if all elements in array are close\n",
    "np.isclose(results_feature_extract_model, results_cloned_model_with_loaded_weights).all() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the layers in the base model and see what dtype policy they're using\n",
    "for layer in cloned_model.layers[1].layers[:20]: # check only the first 20 layers to save space\n",
    "  print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model locally\n",
    "save_dir = \"../models/07_efficientnetb0_feature_extract_model_mixed_precision\"\n",
    "model.save(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from Here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from _helper_functions import create_tensorboard_callback\n",
    "\n",
    "# Load model previously saved above\n",
    "save_dir = \"../models/07_efficientnetb0_feature_extract_model_mixed_precision\"\n",
    "loaded_saved_model = tf.keras.models.load_model(save_dir)\n",
    "(train_data, test_data), ds_info = tfds.load(name=\"food101\",\n",
    "                                            data_dir=\"../Downloads\",\n",
    "                                            split=[\"train\", \"validation\"],\n",
    "                                            shuffle_files=True,\n",
    "                                            as_supervised=True, # data returned as tuple (data, label)\n",
    "                                            with_info=True)\n",
    "                                            # Map preprocessing function to training (parallelize)\n",
    "\n",
    "def preprocess_img(image, label, img_shape=224, normalize=False):\n",
    "    \"\"\"\n",
    "    Converts image datatype to `float32` and reshapes image to [img_shape, img_shape, color_channels]\n",
    "    \"\"\"\n",
    "    image = tf.image.resize(image, [img_shape, img_shape])\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    if normalize:\n",
    "        image = image / 255.0\n",
    "    return image, label\n",
    "                                            \n",
    "train_data = train_data.map(map_func=preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# Shuffle train_data, turn it into batches and prefetch it (load it faster)\n",
    "train_data = train_data.shuffle(buffer_size=1024).batch(batch_size=128).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Map preprocessing function to test data\n",
    "test_data = test_data.map(map_func=preprocess_img, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size=128).prefetch(buffer_size=tf.data.AUTOTUNE).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the layers in the base model and see what dtype policy they're using\n",
    "for layer in loaded_saved_model.layers[1].layers[:20]: # check only the first 20 layers to save output space\n",
    "  print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check loaded model performance (this should be the same as results_feature_extract_model)\n",
    "# results_loaded_saved_model = loaded_saved_model.evaluate(test_data)\n",
    "# results_loaded_saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loaded model's results should equal (or at least be very close) to the model's results prior to saving\n",
    "# Note: this will only work if you've instatiated results variables \n",
    "import numpy as np\n",
    "# Evaluate on whole dataset\n",
    "#np.isclose(results_feature_extract_model, results_loaded_saved_model).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the model's layers for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_saved_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make all layers trainable/unfreeze\n",
    "for layer in loaded_saved_model.layers:\n",
    "  layer.trainable = True # set all layers to trainable\n",
    "  print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy) # make sure loaded model is using mixed precision dtype_policy (\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the layers in the base model and see what dtype policy they're using\n",
    "for layer in loaded_saved_model.layers[1].layers[:20]:\n",
    "  print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping Callback and Model Checkpoint Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup EarlyStopping callback to stop training if model's val_loss doesn't improve for 3 epochs\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", # watch the val loss metric\n",
    "                                                  patience=3) # if val loss decreases for 3 epochs in a row, stop training\n",
    "\n",
    "# Create ModelCheckpoint callback to save best model during fine-tuning\n",
    "checkpoint_path = \"../checkpoints/food_101_milestone/fine_tune_checkpoints.ckpt\"\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                      save_best_only=True,\n",
    "                                                      monitor=\"val_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Reduction Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating learning rate reduction callback\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",  \n",
    "                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n",
    "                                                 patience=2,\n",
    "                                                 verbose=1, # print out when learning rate goes down \n",
    "                                                 min_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "loaded_saved_model.compile(loss=\"sparse_categorical_crossentropy\", # sparse_categorical_crossentropy for labels that are *not* one-hot\n",
    "                        optimizer=tf.keras.optimizers.Adam(0.0001), # 10x lower learning rate than the default\n",
    "                        metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start to fine-tune (all layers)\n",
    "history_101_food_classes_all_data_fine_tune = loaded_saved_model.fit(train_data,\n",
    "                                                        epochs=100, # fine-tune for a maximum of 100 epochs\n",
    "                                                        steps_per_epoch=len(train_data),\n",
    "                                                        validation_data=test_data,\n",
    "                                                        validation_steps=int(0.15 * len(test_data)), # validation during training on 15% of test data\n",
    "                                                        callbacks=[\n",
    "                                                            create_tensorboard_callback(\n",
    "                                                                dir_name=\"../training_logs\", \n",
    "                                                                experiment_name=\"efficientv2b0_101_classes_all_data_fine_tuning\",\n",
    "                                                            ), # track the model training logs\n",
    "                                                            model_checkpoint, # save only the best model during training\n",
    "                                                            early_stopping, # stop model after X epochs of no improvements\n",
    "                                                            reduce_lr\n",
    "                                                            ]\n",
    "                                                        ) # reduce the learning rate after X epochs of no improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_saved_model.evaluate(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('miniconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "215bc93be91779c90c22d9452977b07e46a2652f59eedc500a409df416ed83eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
