{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning with TensorFlow Part 2\n",
    "\n",
    "Fine-tuning within transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check gpu\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Functions\n",
    "\n",
    "from _helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, walk_through_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get some data\n",
    "\n",
    "link: https://www.tensorflow.org/api_docs/python/tf/keras/applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 10% of training data of 10 classes of food 101 data.\n",
    "# Download data\n",
    "!wget -nc -P ../Downloads/ https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
    "\n",
    "# Unzip\n",
    "unzip_data('../Downloads/10_food_classes_10_percent.zip', '../Downloads')\n",
    "\n",
    "# Check number of images and subdirectories in the dataset\n",
    "walk_through_dir('../Downloads/10_food_classes_10_percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Create training and test directory paths\n",
    "train_dir = '../Downloads/10_food_classes_10_percent/train'\n",
    "test_dir = '../Downloads/10_food_classes_10_percent/test'\n",
    "\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir,\n",
    "                                                                            batch_size=BATCH_SIZE,\n",
    "                                                                            image_size=IMG_SIZE,\n",
    "                                                                            label_mode='categorical')\n",
    "                                                                            \n",
    "test_data = tf.keras.preprocessing.image_dataset_from_directory(directory=test_dir,\n",
    "                                                                            batch_size=BATCH_SIZE,\n",
    "                                                                            image_size=IMG_SIZE,\n",
    "                                                                            label_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_10_percent.class_names)\n",
    "# see a batch of data\n",
    "for images, labels in train_data_10_percent.take(1):\n",
    "    print(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a model with the Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model, applications\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "# Creating a model with the Functional API\n",
    "base_model = applications.EfficientNetB0(include_top=False)\n",
    "\n",
    "# Freeze the base model (underlying pre-trained patterns aren't updated during training)\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create inputs into our model\n",
    "inputs = layers.Input(shape=(224, 224, 3), name=\"input_layer\")\n",
    "\n",
    "# Optional param: If using ResNet50V2 you will need to normalize inputs\n",
    "# Not necessary for EfficientNet(s) it has rescaling built in if coming from applications\n",
    "# x = layers.experimental.preprocessing.Rescaling(1./255)(inputs)\n",
    "\n",
    "# Pass the inputs\n",
    "x = base_model(inputs)\n",
    "print(f'shape after passing inputs through base model: {x.shape}')\n",
    "\n",
    "# Average pool the outputs of the base model \n",
    "# (aggregate all the most important pieces of information, reduce number of computations)\n",
    "x = layers.GlobalAveragePooling2D(name=\"global_avg_pooling_layer\")(x)\n",
    "print(f'shape after GlobalAveragePooling2D: {x.shape}')\n",
    "\n",
    "# Create the output activation layer\n",
    "outputs = layers.Dense(10, activation='softmax', name=\"output_layer\")(x)\n",
    "\n",
    "# Combine inputs and outputs into a model\n",
    "model_0 = Model(inputs=inputs, outputs=outputs, name=\"model_0\")\n",
    "\n",
    "# Commpile\n",
    "model_0.compile(optimizer=Adam(),\n",
    "                loss=CategoricalCrossentropy(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model_0_history = model_0.fit(train_data_10_percent,\n",
    "                            epochs=5,\n",
    "                            steps_per_epoch=len(train_data_10_percent),\n",
    "                            validation_data=test_data,\n",
    "                            validation_steps=int(0.25 * len(test_data)),\n",
    "                            callbacks=create_tensorboard_callback(dir_name=\"../tensorflow_hub\",\n",
    "                                                                experiment_name=\"10_percent_feature_extraction\")\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the full test dataset\n",
    "model_0.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the layers in the base model\n",
    "for layers_number, layer in enumerate(base_model.layers):\n",
    "    print(f'{layers_number}: {layer.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print summary of the base model\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how about a summary of our whole model?\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the models training curves\n",
    "plot_loss_curves(model_0_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a feature vector from a trained model\n",
    "\n",
    "Demonstrate the Global Average Pooling 2D layer:\n",
    "\n",
    "We have tensor after our model goes through `base_model` of shape (None, 7, 7, 1280).\n",
    "\n",
    "Then when it passes through `GlobalAveragePooling2D`, it turns into (None, 1280).\n",
    "\n",
    "Let's use a similar shaped tensor of (1, 4, 4, 3) and then pass it to `GlobalAveragePooling2D`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input shape\n",
    "input_shape = (1, 4, 4, 3)\n",
    "\n",
    "# Create a random tensor\n",
    "tf.random.set_seed(42)\n",
    "input_tensor = tf.random.normal(input_shape)\n",
    "print(f'Random input tensor:\\n {input_tensor}\\n')\n",
    "\n",
    "# Pass the random tensor through a global average pooling 2D layer\n",
    "global_avg_pooled_tensor = layers.GlobalAveragePooling2D()(input_tensor)\n",
    "print(f'Global average pooling 2D layer:\\n {global_avg_pooled_tensor}\\n')\n",
    "\n",
    "# Check the shape of the different tensors\n",
    "print(f'Shape of input tensor: {input_tensor.shape}')\n",
    "print(f'Shape of global average pooled tensor: {global_avg_pooled_tensor.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate the Global Average Pooling 2D layer\n",
    "tf.reduce_mean(input_tensor, axis=[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the random tensor through a global max pooling 2D layer\n",
    "global_max_pooled_tensor = layers.GlobalMaxPooling2D()(input_tensor)\n",
    "print(f'Global max pooling 2D layer:\\n {global_max_pooled_tensor}\\n')\n",
    "print(f'Shape of global max pooled tensor: {global_max_pooled_tensor.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a series of transfer learning experiments\n",
    "\n",
    "How does transfer learning work with 1% of the training data?\n",
    "\n",
    "0. `model_0` - baseline model\n",
    "\n",
    "1. `model_1` - use feature extraction transfer learning with 1% of the training data with data augmentation\n",
    "2. `model_2` - use feature extraction transfer learning with 10% of the training data with data augmentation\n",
    "3. `model_3` - use fine-tuning transfer learning with 10% of the training data with data augmentation\n",
    "4. `model_4` - use fine-tuning transfer learning with 100% of the training data with data augmentation\n",
    "\n",
    "**Note:** Throughout all experiments, the same test dataset will be used to evaluate our model.  This ensures consistency aross evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting & preprocessing data for 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and unzip data - preprocessed from Food101\n",
    "!wget -nc -P ../Downloads/ https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_1_percent.zip\n",
    "# Unzip\n",
    "unzip_data('../Downloads/10_food_classes_1_percent.zip', '../Downloads')\n",
    "\n",
    "# Check number of images and subdirectories in the dataset\n",
    "walk_through_dir('../Downloads/10_food_classes_1_percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test directory paths\n",
    "train_dir_1_percent = '../Downloads/10_food_classes_1_percent/train'\n",
    "test_dir_1_percent = '../Downloads/10_food_classes_1_percent/test'\n",
    "\n",
    "# Setup dataloaders\n",
    "train_data_1_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir_1_percent,\n",
    "                                                                            batch_size=BATCH_SIZE,\n",
    "                                                                            image_size=IMG_SIZE,\n",
    "                                                                            label_mode='categorical')\n",
    "                                                                            \n",
    "test_data_1_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=test_dir_1_percent,\n",
    "                                                                            batch_size=BATCH_SIZE,\n",
    "                                                                            image_size=IMG_SIZE,\n",
    "                                                                            label_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding data augmentation\n",
    "\n",
    "To add data augmentation into a model, we can use the layers inside:\n",
    "\n",
    "* `tf.keras.layers.experimental.preprocessing()`\n",
    "\n",
    "When passed as a layer to a model, data augmentation is automatically turned on during training but turned off during inference (does not augment testing data or new unseen data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "# Create data augmentation stage with horizontal flipping, rotations, zooms, etc.\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    data_augmentation = Sequential([\n",
    "        preprocessing.RandomFlip('horizontal'),\n",
    "        preprocessing.RandomRotation(0.2),\n",
    "        preprocessing.RandomZoom(0.2),\n",
    "        preprocessing.RandomHeight(0.2),\n",
    "        preprocessing.RandomWidth(0.2),\n",
    "        # preprocessing.Rescale(1./255) # Keep for models like ResNet50V2 but EfficientNet's have rescaling built-in\n",
    "    ], name=\"data_augmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the data augmentation layer\n",
    "This is because of a recent update to how augmentation layers work in TensorFlow 2.8.\n",
    "\n",
    "A fix should be on the way from the TensorFlow team but for now, one way to fix it is to make sure the parameter training=True is passed to a data augmentation model.\n",
    "\n",
    "This is because data augmentation is only intended to work during training and not testing.\n",
    "\n",
    "Code before\n",
    "This code appears at 5:46 in the next video.\n",
    "\n",
    "augmented_img = data_augmentation(img)\n",
    "\n",
    "Doing this would result in images sometimes not being augmented (changed).\n",
    "\n",
    "Code after the fix\n",
    "augmented_img = data_augmentation(img, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a random image and compare it to its augmented version\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import random\n",
    "\n",
    "target_class = random.choice(train_data_1_percent.class_names)\n",
    "target_dir = '../Downloads/10_food_classes_1_percent/train/' + target_class\n",
    "# print(os.listdir())\n",
    "random_image = random.choice(os.listdir(target_dir))\n",
    "random_image_path = os.path.join(target_dir, random_image)\n",
    "\n",
    "# Read in random image\n",
    "img = mpimg.imread(random_image_path)\n",
    "plt.imshow(img)\n",
    "plt.title(f'Original random image | Class: {target_class}')\n",
    "plt.axis(False)\n",
    "\n",
    "# Plot our augmented random image\n",
    "augmented_img = data_augmentation(tf.expand_dims(img, axis=0), training=True)\n",
    "plt.figure()\n",
    "plt.imshow(tf.squeeze(augmented_img)/255.)\n",
    "plt.title(f'Augmented random image | Class: {target_class}')\n",
    "plt.axis(False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - Feature extraction 1% data augmentation\n",
    "#### Feature extraction transfer learning on 1% of the data with augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup input shape and base model, freeze base model layers\n",
    "\n",
    "input_shape = (IMG_SIZE + (3,))\n",
    "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create input layer\n",
    "inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n",
    "\n",
    "# Add data augmentation Sequential model as a layer\n",
    "x = data_augmentation(inputs) # Augment our training images (augmentation doesn't occur on test data)\n",
    "\n",
    "# Give base_model the inputs (after augmentation) and don't train it\n",
    "x = base_model(x, training=False) \n",
    "# Pass augmented images through base_model but keep it in inference mode, \n",
    "# this insures batchnorm layers don't update\n",
    "\n",
    "# Pool output features of the base_model\n",
    "x = layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
    "\n",
    "# Put a dense layer on as the output\n",
    "outputs = layers.Dense(10, activation='softmax', name=\"output_layer\")(x)\n",
    "\n",
    "# Make a model using the inputs and outputs\n",
    "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1\")\n",
    "\n",
    "# Compile the model\n",
    "model_1.compile(loss=CategoricalCrossentropy(),\n",
    "                optimizer=Adam(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history_model_1 = model_1.fit(train_data_1_percent,\n",
    "                                epochs=5,\n",
    "                                steps_per_epoch=len(train_data_1_percent),\n",
    "                                validation_data=test_data, \n",
    "                                validation_steps=int(0.25 * len(test_data)),\n",
    "                                # Track model training logs\n",
    "                                callbacks=[create_tensorboard_callback(dir_name=\"../tensorflow_hub\",\n",
    "                                                                experiment_name=\"1_percent_data_aug\")]\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate and check loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the model's summary\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the full test dataset\n",
    "results_1_percent_data_aug = model_1.evaluate(test_data, steps=len(test_data))\n",
    "results_1_percent_data_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(history_model_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - Feature extraction 10% data augmentation\n",
    "#### Feature extraction transfer learning on 10% of the data with augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Model, applications\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, GlobalAveragePooling2D, Input\n",
    "# Same as model 1 but with 10% of data\n",
    "IMG_SIZE = (224, 224)\n",
    "input_shape = (IMG_SIZE + (3,))\n",
    "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create input layer\n",
    "inputs = Input(shape=input_shape, name=\"input_layer\")\n",
    "\n",
    "# Add data augmentation Sequential model as a layer\n",
    "x = data_augmentation(inputs) # Augment our training images (augmentation doesn't occur on test data)\n",
    "\n",
    "# Give base_model the inputs (after augmentation) and don't train it\n",
    "x = base_model(x, training=False) \n",
    "# Pass augmented images through base_model but keep it in inference mode, \n",
    "# this insures batchnorm layers don't update\n",
    "\n",
    "# Pool output features of the base_model\n",
    "x = GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
    "\n",
    "# Put a dense layer on as the output\n",
    "outputs = Dense(10, activation='softmax', name=\"output_layer\")(x)\n",
    "\n",
    "\n",
    "# Make a model using the inputs and outputs\n",
    "model_2 = tf.keras.Model(inputs, outputs, name=\"model_2\")\n",
    "\n",
    "# Compile the model\n",
    "model_2.compile(loss=CategoricalCrossentropy(),\n",
    "                optimizer=Adam(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ModelCheckpoint callback\n",
    "\n",
    "**Model Checkpointing:** Save your model as it trains so you can stop training if needed and come back to continue where you left off.  Helpful if training takes a long time and can't be done in one sitting.\n",
    "\n",
    "`tf.keras.callbacks.ModelCheckpoint`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set checkpoint path\n",
    "checkpoint_path = \"../checkpoints/10_percent_model_checkpoint_weights/checpoint.ckpt\"\n",
    "\n",
    "# Create a ModelCheckpoint callback that saves the model's weights only\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                                save_weights_only=True,\n",
    "                                                                save_best_only=False,\n",
    "                                                                save_freq=\"epoch\", #default is save every epoch,\n",
    "                                                                verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit model 2 passing in the ModelCheckpoint callback1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model saving checkpoints every epoch\n",
    "initial_epochs = 5\n",
    "# Fit the model\n",
    "history_model_2 = model_2.fit(train_data_10_percent,\n",
    "                                epochs=initial_epochs,\n",
    "                                steps_per_epoch=len(train_data_10_percent),\n",
    "                                validation_data=test_data, \n",
    "                                validation_steps=int(0.25 * len(test_data)),\n",
    "                                # Track model training logs\n",
    "                                callbacks=[create_tensorboard_callback(dir_name=\"../tensorflow_hub\",\n",
    "                                                                experiment_name=\"10_percent_data_aug\"), \n",
    "                                            model_checkpoint_callback\n",
    "                                    ]\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 0 results\n",
    "results_model_0 = model_0.evaluate(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 10 percent results\n",
    "results_10_percent_data_aug = model_2.evaluate(test_data)\n",
    "results_10_percent_data_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model loss curves\n",
    "plot_loss_curves(history_model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in checkpointed weights\n",
    "\n",
    "Returns a model to a specific checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "model_2.load_weights(checkpoint_path)\n",
    "\n",
    "# Evaluate loaded weights model\n",
    "results_loaded_weights = model_2.evaluate(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing loaded weights\n",
    "\n",
    "If the results from our previously evaluated model_2 match the loaded weights, everything has worked.\n",
    "However, to check equality, the comparison has to be approximate.\n",
    "\n",
    "Use `np.isclose()` to compare the results.\n",
    "\n",
    "<u>**Parameters:**</u>\n",
    "\n",
    "    a, b (array_like):\n",
    "\n",
    "Input arrays to compare.\n",
    "\n",
    "    rtol (float):\n",
    "\n",
    "The relative tolerance parameter (see Notes).\n",
    "\n",
    "    atol (float):\n",
    "\n",
    "The absolute tolerance parameter (see Notes).\n",
    "\n",
    "    equal_nan (bool):\n",
    "\n",
    "Whether to compare NaN’s as equal. If True, NaN’s in a will be considered equal to NaN’s in b in the output array.\n",
    "\n",
    "<u>**Returns:**</u>\n",
    "\n",
    "    y (array_like):\n",
    "\n",
    "Returns a boolean array of where a and b are equal within the given tolerance. If both a and b are scalars, returns a single boolean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "comparison = np.isclose(results_10_percent_data_aug, results_loaded_weights, atol=0.0001)\n",
    "print(comparison)\n",
    "\n",
    "# Actual difference \n",
    "print(np.array(results_10_percent_data_aug)-np.array(results_loaded_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 -  Fine Tuning\n",
    "\n",
    "Use fine-tuning transfer learning with 10% of the training data with data augmentation\n",
    "\n",
    "**Note:** Fine tuning usually works best *after* training a feature extraction model for a few epochs with large amounts of custom data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers in our loaded model\n",
    "model_2.layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which layers (if any) are trainable\n",
    "\n",
    "for layer in model_2.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print layer info for base model (EfficientNetB0)\n",
    "\n",
    "for i, layer in enumerate(model_2.layers[2].layers):\n",
    "    print(i, layer.name, f'\\n\\tTrainable: {layer.trainable}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many trainable variables are in the base model?\n",
    "print(len(model_2.layers[2].trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make top 10 layers of base model trainable\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze all layers except for the top 10 layers\n",
    "for layer in base_model.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile the model to apply the changes\n",
    "base_model.compile(loss=CategoricalCrossentropy(), optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enable training on specified layers\n",
    "**Note:** When fine tuning, lower learning rate to avoid overfitting. Typically 10x lower than original learning rate though different sources will claim other values.\n",
    "\n",
    "A good resource for info on this is the UMLFIT paper:\n",
    "https://arxiv.org/pdf/1801.06146v5.pdf\n",
    "\n",
    "Alternatively, dynamically change the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Trainable Variables: {len(model_2.layers[2].trainable_variables)}')\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    if (layer.trainable):\n",
    "        print(f'Index: {i}\\n\\tName: {layer.name}\\n\\tTrainable: {layer.trainable}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune for another 5 epochs\n",
    "initial_epochs = 5 # reinitialized to 5\n",
    "fine_tune_epochs = initial_epochs + 5\n",
    "\n",
    "# Refit the model (same as model_2 except with more trainable layers)\n",
    "history_fine_10_percent = model_2.fit(train_data_10_percent, \n",
    "    epochs=fine_tune_epochs,\n",
    "    initial_epoch=history_model_2.epoch[-1], # start training from previous last epoch\n",
    "    steps_per_epoch=len(train_data_10_percent), \n",
    "    validation_data=test_data, \n",
    "    validation_steps=int(0.25 * len(test_data)), \n",
    "    callbacks=[\n",
    "        create_tensorboard_callback(dir_name=\"../tensorflow_hub\", experiment_name=\"10_percent_fine_tune_last_10\"), \n",
    "        model_checkpoint_callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the fine-tuned model (model_3 which is actually model_2 fine-tuned for another 5 epochs)\n",
    "\n",
    "results_fine_tune_10_percent = model_2.evaluate(test_data)\n",
    "results_fine_tune_10_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(history_fine_10_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare histories\n",
    "The `plot_loss_curves` function works great with models which have only been fit once, however, we want something to compare one series of running `fit()` with another.\n",
    "\n",
    "e.g. before and after fine-tuning (compare history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_histories(original_history, new_history, initial_epochs=5):\n",
    "    \"\"\"\n",
    "    Compares two TensorFlow History Objects\n",
    "\n",
    "    Args:\n",
    "        original_history (tf history object):\n",
    "            Original history object.\n",
    "        new_history (tf history object):\n",
    "            Fine tuned history object.\n",
    "        initial_epochs (int):\n",
    "            Number of epochs trained on original model.\n",
    "\n",
    "    Returns:\n",
    "        plots comparing total loss and accuracy for original and fine-tuned model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Get original history measurments\n",
    "    acc = original_history.history['accuracy']\n",
    "    loss = original_history.history['loss']\n",
    "\n",
    "    val_acc = original_history.history['val_accuracy']\n",
    "    val_loss = original_history.history['val_loss']\n",
    "\n",
    "    # Combine original history with fine-tuned history\n",
    "    total_acc = acc + new_history.history['accuracy']\n",
    "    total_loss = loss + new_history.history['loss']\n",
    "\n",
    "    total_val_acc = val_acc + new_history.history['val_accuracy']\n",
    "    total_val_loss = val_loss + new_history.history['val_loss']\n",
    "\n",
    "    # Make accuracy plots\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(total_acc, label=\"Training Accuracy\")\n",
    "    plt.plot(total_val_acc, label=\"Val Accuracy\")\n",
    "    plt.plot([initial_epochs-1, initial_epochs-1], plt.ylim(), label=\"Start Fine-tuning\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "\n",
    "     # Make loss plots\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(total_loss, label=\"Training Loss\")\n",
    "    plt.plot(total_val_loss, label=\"Val Loss\")\n",
    "    plt.plot([initial_epochs-1, initial_epochs-1], plt.ylim(), label=\"Start Fine-tuning\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(\"Training and Validation Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_histories(history_model_2, history_fine_10_percent, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4 - Fine Tuning with 100% of the training data with augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "!wget -nc -P ../Downloads/ https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip\n",
    "\n",
    "# Unzip\n",
    "unzip_data('../Downloads/10_food_classes_all_data.zip', '../Downloads')\n",
    "\n",
    "# Check number of images and subdirectories in the dataset\n",
    "walk_through_dir('../Downloads/10_food_classes_all_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Create training and test directory paths\n",
    "all_data_train_dir = '../Downloads/10_food_classes_all_data/train'\n",
    "all_data_test_dir = '../Downloads/10_food_classes_all_data/test'\n",
    "\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "train_data_all = tf.keras.preprocessing.image_dataset_from_directory(directory=all_data_train_dir,\n",
    "                                                                            batch_size=BATCH_SIZE,\n",
    "                                                                            image_size=IMG_SIZE,\n",
    "                                                                            label_mode='categorical')\n",
    "                                                                            \n",
    "test_data_all = tf.keras.preprocessing.image_dataset_from_directory(directory=all_data_test_dir,\n",
    "                                                                            batch_size=BATCH_SIZE,\n",
    "                                                                            image_size=IMG_SIZE,\n",
    "                                                                            label_mode='categorical')\n",
    "\n",
    "initial_epochs = 5 # reinitialized to 5\n",
    "fine_tune_epochs = initial_epochs + 5\n",
    "\n",
    "# Refit the model (same as model_2 except with more trainable layers)\n",
    "# history_fine_all_data = model_2.fit(train_data_all, \n",
    "#     epochs=fine_tune_epochs,\n",
    "#     initial_epoch=history_model_2.epoch[-1], # start training from previous last epoch\n",
    "#     steps_per_epoch=len(train_data_all), \n",
    "#     validation_data=test_data_all, \n",
    "#     validation_steps=int(0.25 * len(test_data_all)), \n",
    "#     callbacks=[\n",
    "#         create_tensorboard_callback(dir_name=\"../tensorflow_hub\", experiment_name=\"10_percent_fine_tune_last_10\"), \n",
    "#         model_checkpoint_callback]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_model_2 = model_2.evaluate(test_data)\n",
    "np.isclose(results_model_2, results_fine_tune_10_percent, atol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revert model_2 (technically model 3) back to checkpoint version of model_2\n",
    "model_2.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate loaded weights model\n",
    "model_2.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_10_percent_data_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what happened\n",
    "\n",
    "model_1. Trained a feature extraction transfer learning model for 5 epochs on 10% of the data (with all base model layers frozen) and saved the model's weights using `ModelCheckpoint`.\n",
    "model_2. Fine-tuned the same model on the same 10% of the data for a further 5 epochs with the top 10 layers of the base model unfrozen.\n",
    "model_3. Saved the results and training logs each time.\n",
    "model_4. Reloaded the model from 1 to do the same steps as 2 but with all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Trainable Variables: {len(model_2.layers[2].trainable_variables)}')\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    if (layer.trainable):\n",
    "        print(f'Index: {i}\\n\\tName: {layer.name}\\n\\tTrainable: {layer.trainable}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile \n",
    "model_2.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training model_2 with 100% of the data.\n",
    "fine_tune_epochs = initial_epochs + 5\n",
    "history_fine_10_classes_full = model_2.fit(train_data_all,\n",
    "                                            epochs=fine_tune_epochs,\n",
    "                                            initial_epoch=history_fine_10_percent.epoch[-1],\n",
    "                                            steps_per_epoch=len(train_data_all),\n",
    "                                            validation_data=test_data_all,\n",
    "                                            validation_steps=int(0.25 * len(test_data_all)),\n",
    "                                            callbacks=[\n",
    "                                                create_tensorboard_callback(dir_name=\"../tensorflow_hub\", experiment_name=\"full_10_classes_fine_tune_last_10\"),\n",
    "                                                model_checkpoint_callback]\n",
    "                                            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on all test data\n",
    "results_fine_tune_full_data = model_2.evaluate(test_data_all)\n",
    "results_fine_tune_full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_histories(history_fine_10_percent, history_fine_10_classes_full, initial_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing experiment data on TensorBoard\n",
    "\n",
    "**Note:** TensorBoard is a visualization tool for TensorFlow. Any data uploaded will be public."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading to TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View tensorboard logs of transfer learning modelling experiments\n",
    "# Upload TensorBoard dev records.\n",
    "\n",
    "!tensorboard dev upload --logdir=../tensorflow_hub \\\n",
    "--name \"Transfer Learning Experiment with 10 Food101 Classes\" \\\n",
    "--description \"Series of transfer learning experiments with varying amounts of data and fine-tuning\" \\\n",
    "--one_shot # Exits the uploader once it has finished uploading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard experiments available at: https://tensorboard.dev/experiment/kNcqp4oSQlGAAbALH07NZA/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To delete an experiment\n",
    "`!tensorboard dev delete --experiment_id kNcqp4oSQlGAAbALH07NZA`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('directml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c179f5f00e546169410edbb1f0aaf45df98c7fd11ac06763c585ffbea32bb02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
