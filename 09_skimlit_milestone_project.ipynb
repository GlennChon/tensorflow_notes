{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone Project 2: SkimLit\n",
    "\n",
    "Sequence Problem:\n",
    "Many to One Classification\n",
    "\n",
    "* Download PubMed 200k RCT dataset\n",
    "* Preprocess the text data\n",
    "* Set up multiple modeling experiments\n",
    "* Build a multimodal model to take in different sources of data\n",
    "  * Replicate the model powering https://arxiv.org/abs/1710.06071\n",
    "* Find the most wrong prediction examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skim Medical Literature \n",
    "\n",
    "A Dataset for sequential Sentence Classification in Medical Abastracts:\n",
    "\n",
    "[Source](https://arxiv.org/abs/1710.06071)\n",
    "\n",
    "[Model Architecture](https://arxiv.org/abs/1612.05251)\n",
    "\n",
    "Artificial Neural Network consisting of 3 main components:\n",
    "* Token embedding layer (bi-LSTM)\n",
    "* Sentence label prediction layer (bi-LSTM)\n",
    "* Label sequence optimization layer (CRF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm access to GPU\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset\n",
    "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git ../Downloads/09_skimlit_milestone_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check downloaded folders\n",
    "!ls ../Downloads/09_skimlit_milestone_project\n",
    "\n",
    "# Check files in the one of the folders\n",
    "!ls ../Downloads/09_skimlit_milestone_project/PubMed_20k_RCT_numbers_replaced_with_at_sign/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 20k RCT dataset with numbers replaced with @ sign\n",
    "data_dir = \"../Downloads/09_skimlit_milestone_project/PubMed_20k_RCT_numbers_replaced_with_at_sign/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the filenames in target dir\n",
    "import os\n",
    "filenames = os.listdir(data_dir)\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to read in all the lines of a target text file\n",
    "def get_lines(filepath):\n",
    "    \"\"\"\n",
    "    Reads in a text file and returns a list of lines.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The path to the text file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of lines in the text file.\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\") as f:\n",
    "        return f.readlines()\n",
    "\n",
    "\n",
    "# Check the first 10 lines of the train file\n",
    "train_lines = get_lines(data_dir + \"train.txt\")\n",
    "train_lines[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structuring the data\n",
    "\n",
    "```\n",
    "[\n",
    "    {\n",
    "        'line_number': 0, \n",
    "        'target': 'BACKGROUND',\n",
    "        'text': 'Emotional eating is associated with overeating and the development of obesity .\\n',\n",
    "        'total_lines': 11, \n",
    "    }, \n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that takes a list of lines and returns a list of dictionarie\n",
    "def create_dicts(filepath):\n",
    "    \"\"\"\n",
    "    Creates a list of dictionaries of abstract line data\n",
    "\n",
    "    Args:\n",
    "        filepath.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries with the keys \"line_number\", \"target\", \"text\", \"total_lines\", abstract_id.\n",
    "    \"\"\"\n",
    "    abstract_lines = \"\" # Create an empty abstract\n",
    "    abstract_samples = [] # Create an empty list of abstract samples\n",
    "    input_lines = get_lines(filepath)\n",
    "\n",
    "    for i, line in enumerate(input_lines):\n",
    "        if line.startswith(\"###\"):\n",
    "            abstract_id = line\n",
    "            abstract_lines = \"\" # reset the abstract string if the line is an ID line\n",
    "        elif line.isspace(): \n",
    "            # if line is end of abstract, take abstract lines and create a dictionary, \n",
    "            # then append the dictionary to abstract_samples\n",
    "            abstract_line_split = abstract_lines.splitlines() # split abstract lines on new line\n",
    "            # Iterate through each line in a single abstract and keep count\n",
    "            for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
    "                line_split = abstract_line.split(\"\\t\")\n",
    "                # Create a dictionary of the line data\n",
    "                abstract_sample = {\n",
    "                    \"line_number\": abstract_line_number,\n",
    "                    \"target\": line_split[0],\n",
    "                    \"text\": line_split[1].lower(),\n",
    "                    \"total_lines\": len(abstract_line_split) - 1,\n",
    "                    \"abstract_id\": abstract_id\n",
    "                }\n",
    "                abstract_samples.append(abstract_sample)\n",
    "        else: # appends line to abstract lines if the end of the abstract is not reached\n",
    "            abstract_lines += line\n",
    "\n",
    "  \n",
    "    return abstract_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from each file and preprocess it\n",
    "train_samples = create_dicts(data_dir + \"train.txt\")\n",
    "val_samples = create_dicts(data_dir + \"dev.txt\")\n",
    "test_samples = create_dicts(data_dir + \"test.txt\")\n",
    "len(train_samples),  len(val_samples), len(test_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.DataFrame(train_samples)\n",
    "val_df = pd.DataFrame(val_samples)\n",
    "test_df = pd.DataFrame(test_samples)\n",
    "train_df.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of labels in training data\n",
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check length of different lines\n",
    "train_df.total_lines.plot.hist()\n",
    "plt.title(\"Distribution of total lines in abstracts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get lists of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert abstract text lines into lists\n",
    "train_sentences = train_df.text.tolist()\n",
    "val_sentences = val_df.text.tolist()\n",
    "test_sentences = test_df.text.tolist()\n",
    "len(train_sentences), len(val_sentences), len(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 10 lines of the training data\n",
    "train_sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make numeric labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Onehot encoded labels\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "# alternatively use tf.one_hot\n",
    "one_hot_encoder = OneHotEncoder(sparse=False) # We want a non-sparse matrix\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_df.target.to_numpy().reshape(-1, 1))\n",
    "val_labels_one_hot = one_hot_encoder.transform(val_df.target.to_numpy().reshape(-1, 1))\n",
    "test_labels_one_hot = one_hot_encoder.transform(test_df.target.to_numpy().reshape(-1, 1))\n",
    "# Check what one_hot encoded labels look like\n",
    "train_labels_one_hot\n",
    "tf.constant(train_labels_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels (\"target\" columns) and encode them into integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_df.target.to_numpy())\n",
    "val_labels_encoded = label_encoder.transform(val_df.target.to_numpy())\n",
    "test_labels_encoded = label_encoder.transform(test_df.target.to_numpy())\n",
    "# Check what encoded labels look like\n",
    "train_labels_encoded[:12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class names and number of classes from LabelEncoder instance\n",
    "num_classes = len(label_encoder.classes_)\n",
    "class_names = label_encoder.classes_\n",
    "num_classes, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series of Experiments\n",
    "\n",
    "* 0 - Naive Bayes with TF-IDF encoder (baseline)\n",
    "* 1 - Conv1D with token embeddings\n",
    "* 2 - TF Hub Pretrained Feature Extractor\n",
    "* 3 - Conv1D with character embeddings\n",
    "* 4 - Combining pretrained token embeddings + characters embeddings (hybrid embedding layer)\n",
    "* 5 - Combining pretrained token embeddings + characters embeddings + positional embeddings\n",
    "\n",
    "[Machine Learning Testing Map](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 0: Baseline Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create tokenization and modelling pipeline\n",
    "model_0 = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n",
    "    (\"clf\", MultinomialNB()), # model the text using a naive bayes classifier\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(train_sentences, \n",
    "            train_labels_encoded, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate our baseline model\n",
    "baseline_score =  model_0.score(val_sentences, val_labels_encoded)\n",
    "print(f'Baseline accuracy score: {baseline_score * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using baseline model\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use helper functions script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _helper_functions import calculate_results\n",
    "# Calculate baseline results\n",
    "baseline_results = calculate_results(y_true=val_labels_encoded,\n",
    "                                        y_pred=baseline_preds)\n",
    "\n",
    "baseline_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Find the average number of tokens (words) in the training sentences\n",
    "sent_lens = [len(sentence.split()) for sentence in train_sentences]\n",
    "avg_sent_len = np.mean(sent_lens)\n",
    "print(avg_sent_len)\n",
    "\n",
    "# What's the distribution look like?\n",
    "plt.hist(sent_lens, bins=20)\n",
    "plt.title(\"Distribution of sentence length\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How long of a sentence length covers 95% of the examples?\n",
    "output_seq_len = int(np.percentile(sent_lens, 95))\n",
    "print(output_seq_len, \"tokens (words) or less covers 95% of training examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Conv1D with token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize train sentences, turn it into an embedding and build a model\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import random\n",
    "\n",
    "# Setup text vectorization variables\n",
    "max_vocab_len = 68000 # 68k value used in paper for PubMed 20k RCT, 200k is 331k\n",
    "\n",
    "# Create text vectorizer\n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab_len,\n",
    "                                    output_sequence_length=output_seq_len)\n",
    "                                    \n",
    "# Adapt the text vectorizer to the training texts\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize random sentence & view\n",
    "target_sentence = random.choice(train_sentences)\n",
    "vectorized_sentence = text_vectorizer([target_sentence])\n",
    "print(f\"Text:\\n{target_sentence}\")\n",
    "print(f\"\\nLength of text: {len(target_sentence.split())}\")\n",
    "print(f\"\\nVectorized text: {vectorized_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore: How many words are in the vocabulary?\n",
    "rct_20k_text_vocab = text_vectorizer.get_vocabulary()\n",
    "print(f\"Number of words in vocab: {len(rct_20k_text_vocab)}\")\n",
    "print(f\"Most common words: {rct_20k_text_vocab[:5]}\")\n",
    "print(f\"Least common words: {rct_20k_text_vocab[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the config of our text vectorizer\n",
    "text_vectorizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "# Create an embedding layer\n",
    "token_embed = layers.Embedding(input_dim=len(rct_20k_text_vocab), # length of vocab\n",
    "                                output_dim=128, # size of embedding vectors\n",
    "                                mask_zero=True, # whether or not the input value 0 is a special \"padding\" value which should be masked out\n",
    "                                name=\"token_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example embedding\n",
    "\n",
    "print(f\"Original text:\\n {target_sentence}\\\n",
    "        \\n\\nVectorized version: {vectorized_sentence}\")  \n",
    "embedded_sentence = token_embed(vectorized_sentence)\n",
    "print(f\"\\nEmbedded version: {embedded_sentence}\")\n",
    "print(f\"\\nEmbedded version shape: {embedded_sentence.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets with the tf.data API\n",
    "# Turn the data into TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot))\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the TensorFlow datasets and turn them into prefetched batches\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 1D conv model with token embedding to process sequences\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "text_vectors = text_vectorizer(inputs)\n",
    "token_embeddings = token_embed(text_vectors)\n",
    "x = layers.Conv1D(64, 5, activation=\"relu\")(token_embeddings)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_conv1D\")\n",
    "\n",
    "# Get a summary of our model\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _helper_functions import create_tensorboard_callback\n",
    "#Compile model\n",
    "model_1.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history_1 = model_1.fit(train_dataset,\n",
    "                        steps_per_epoch=int(0.1*len(train_dataset)),\n",
    "                        epochs=3,\n",
    "                        validation_data=val_dataset,\n",
    "                        validation_steps=int(0.1*len(val_dataset)), # only validates on 10% of validation data\n",
    "                        callbacks=[create_tensorboard_callback(dir_name=\"../tensorflow_hub/skimlit\",\n",
    "                                                                experiment_name=\"model_1_conv1D\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "model_1_pred_probs = model_1.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_pred_probs, model_1_pred_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert prediction probabilities to class labels\n",
    "model_1_preds = tf.argmax(model_1_pred_probs, axis=1)\n",
    "model_1_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate model results\n",
    "model_1_results = calculate_results(y_true=val_labels_encoded,\n",
    "                                    y_pred=model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: TF Hub Pretrained Feature Extractor\n",
    "\n",
    "[Universal Sentence Encoder v4 (USE)](https://tfhub.dev/google/universal-sentence-encoder/4)\n",
    "\n",
    "The paper originally used Global Vectors for Word Representation (GloVe) embeddings.\n",
    "\n",
    "This notebook uses the latest USE pretrained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF Hub pretrained feature extractor\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Create Keras layer using pretrained feature extractor\n",
    "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        input_shape=[], # can take variable length sequences\n",
    "                                        dtype=tf.string, # accepts string inputs\n",
    "                                        trainable=False, # freeze the pretrained weights\n",
    "                                        name=\"USE_feature_extractor_layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out the pretrained embedding on a random sentence\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n {random_sentence}\")\n",
    "use_embedded_sentence = sentence_encoder_layer([random_sentence])\n",
    "print(f\"\\nEmbedded version: {use_embedded_sentence[:30]}\")\n",
    "print(f\"\\nEmbedded version length: {len(use_embedded_sentence[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature extraction model using TF Hub layer\n",
    "inputs = layers.Input(shape=[], dtype=tf.string)\n",
    "pretrained_embedding = sentence_encoder_layer(inputs)\n",
    "x = layers.Dense(128, activation=\"relu\")(pretrained_embedding)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "model_2 = tf.keras.Model(inputs, outputs, name=\"model_2_USE_feature_extractor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_2.compile(loss=\"categorical_crossentropy\",\n",
    "                    optimizer=tf.keras.optimizers.Adam(),\n",
    "                    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "history_2 = model_2.fit(train_dataset,\n",
    "                        steps_per_epoch=int(0.1*len(train_dataset)),\n",
    "                        epochs=3,\n",
    "                        validation_data=val_dataset,\n",
    "                        validation_steps=int(0.1*len(val_dataset)),\n",
    "                        callbacks=[\n",
    "                            create_tensorboard_callback(dir_name=\"../tensorflow_hub/skimlit\",\n",
    "                                experiment_name=\"model_2_USE\")\n",
    "                            ]\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "model_2_pred_probs = model_2.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert prediction probabilities to class labels\n",
    "model_2_preds = tf.argmax(model_2_pred_probs, axis=1)\n",
    "model_2_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate model results\n",
    "model_2_results = calculate_results(y_true=val_labels_encoded,\n",
    "                                    y_pred=model_2_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results, model_1_results, model_2_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Conv1D with character embeddings\n",
    "\n",
    "Same as model 1 but use character-level embeddings.\n",
    "\n",
    "The paper used a combination of token and character embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to split sentences\n",
    "def split_chars(text):\n",
    "    return \" \".join(list(text))\n",
    "\n",
    "# Text splitting non-character-level sequence into characters\n",
    "split_chars(random_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sequence-level data splits into character-level data splits\n",
    "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
    "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
    "test_chars = [split_chars(sentence) for sentence in test_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get avg char length for each sentence in train_sentences\n",
    "char_lens = [len(sentence) for sentence in train_sentences]\n",
    "mean_char_len = np.mean(char_lens)\n",
    "mean_char_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of character lengths\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(char_lens, bins=10)\n",
    "plt.xticks(range(0,1500,100))\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find what character length covers 95% of sequences\n",
    "output_seq_char_len = int(np.percentile(char_lens, 95))\n",
    "output_seq_char_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "alphabet = string.ascii_lowercase # + string.digits + string.punctuation #not needed if using default standardizer\n",
    "alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create char-level token vectorizer instance\n",
    "NUM_CHAR_TOKENS = len(alphabet) + 2 # add 2 for OOV [UNK] and space tokens\n",
    "char_vectorizer = TextVectorization(max_tokens=NUM_CHAR_TOKENS,\n",
    "                                    output_sequence_length=output_seq_char_len,\n",
    "                                    # standardize=None,\n",
    "                                    name=\"char_vectorizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt the char_vectorizer to our training characters\n",
    "char_vectorizer.adapt(train_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check character vocab stats\n",
    "char_vocab = char_vectorizer.get_vocabulary()\n",
    "print(f\"Number of different chars in char vocab: {len(char_vocab)}\")\n",
    "print(f\"5 Most common chars in char vocab: {char_vocab[:5]}\")\n",
    "print(f\"5 Least common chars in char vocab: {char_vocab[-5:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out character vectorizer\n",
    "random_train_chars = random.choice(train_chars)\n",
    "print(f\"Original text:\\n {random_train_chars}\")\n",
    "print(f\"\\nLength of original text: {len(random_train_chars)}\")\n",
    "\n",
    "vectorized_chars = char_vectorizer([random_train_chars])\n",
    "print(f\"\\nVectorized version: {vectorized_chars}\")\n",
    "print(f\"\\nLength of vectorized version: {len(vectorized_chars[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a character-level embedding\n",
    "char_embed = layers.Embedding(input_dim=len(char_vocab),\n",
    "                                output_dim=25,\n",
    "                                mask_zero=True, # add masking to account for OOV tokens\n",
    "                                name=\"char_embedding\")\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out character embedding\n",
    "print(f\"Char split sentences:\\n {random_train_chars}\\n\")\n",
    "char_embed_example = char_embed(char_vectorizer([random_train_chars]))\n",
    "print(f\"Embedded version: {char_embed_example}\")\n",
    "print(f\"Embedded version shape: {char_embed_example.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a conv1D character-level embedding model\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "char_vectors = char_vectorizer(inputs)\n",
    "char_embeddings = char_embed(char_vectors)\n",
    "x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(char_embeddings)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "model_3 = tf.keras.Model(inputs, outputs, name=\"model_3_conv1D_char_embedding\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "model_3.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit  \n",
    "# TODO: Need to optimize data before fit\n",
    "\n",
    "# history_3 = model_3.fit(train_dataset,\n",
    "#                         steps_per_epoch=int(0.1*len(train_dataset)),\n",
    "#                         epochs=3,\n",
    "#                         validation_data=val_dataset,\n",
    "#                         validation_steps=int(0.1*slen(val_dataset)),\n",
    "#                         callbacks=[\n",
    "#                             create_tensorboard_callback(dir_name=\"../tensorflow_hub/skimlit\",\n",
    "#                                 experiment_name=\"model_3_char_embedding\")\n",
    "#                             ]\n",
    "#                         )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eef4eb45b5ce6f96548309064ae87146eaf6cbe4b05ab916c771d60a96931cae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
