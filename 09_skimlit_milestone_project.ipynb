{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone Project 2: SkimLit\n",
    "\n",
    "Sequence Problem:\n",
    "Many to One Classification\n",
    "\n",
    "* Download PubMed 200k RCT dataset\n",
    "* Preprocess the text data\n",
    "* Set up multiple modeling experiments\n",
    "* Build a multimodal model to take in different sources of data\n",
    "  * Replicate the model powering https://arxiv.org/abs/1710.06071\n",
    "* Find the most wrong prediction examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skim Medical Literature \n",
    "\n",
    "A Dataset for sequential Sentence Classification in Medical Abastracts:\n",
    "\n",
    "[Source](https://arxiv.org/abs/1710.06071)\n",
    "\n",
    "[Model Architecture](https://arxiv.org/abs/1612.05251)\n",
    "\n",
    "Artificial Neural Network consisting of 3 main components:\n",
    "* Token embedding layer (bi-LSTM)\n",
    "* Sentence label prediction layer (bi-LSTM)\n",
    "* Label sequence optimization layer (CRF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 3070 Laptop GPU (UUID: GPU-a72a565b-b891-b764-c0f7-6096f5184f0e)\n"
     ]
    }
   ],
   "source": [
    "#Confirm access to GPU\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path '../Downloads/09_skimlit_milestone_project' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# Get dataset\n",
    "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git ../Downloads/09_skimlit_milestone_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PubMed_200k_RCT\n",
      "PubMed_200k_RCT_numbers_replaced_with_at_sign\n",
      "PubMed_20k_RCT\n",
      "PubMed_20k_RCT_numbers_replaced_with_at_sign\n",
      "README.md\n",
      "dev.txt  test.txt  train.txt\n"
     ]
    }
   ],
   "source": [
    "# Check downloaded folders\n",
    "!ls ../Downloads/09_skimlit_milestone_project\n",
    "\n",
    "# Check files in the one of the folders\n",
    "!ls ../Downloads/09_skimlit_milestone_project/PubMed_20k_RCT_numbers_replaced_with_at_sign/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 20k RCT dataset with numbers replaced with @ sign\n",
    "data_dir = \"../Downloads/09_skimlit_milestone_project/PubMed_20k_RCT_numbers_replaced_with_at_sign/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dev.txt', 'test.txt', 'train.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the filenames in target dir\n",
    "import os\n",
    "filenames = os.listdir(data_dir)\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['###24293578\\n',\n",
       " 'OBJECTIVE\\tTo investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .\\n',\n",
       " 'METHODS\\tA total of @ patients with primary knee OA were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .\\n',\n",
       " 'METHODS\\tOutcome measures included pain reduction and improvement in function scores and systemic inflammation markers .\\n',\n",
       " 'METHODS\\tPain was assessed using the visual analog pain scale ( @-@ mm ) .\\n',\n",
       " 'METHODS\\tSecondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and @-min walk distance ( @MWD ) .\\n',\n",
       " 'METHODS\\tSerum levels of interleukin @ ( IL-@ ) , IL-@ , tumor necrosis factor ( TNF ) - , and high-sensitivity C-reactive protein ( hsCRP ) were measured .\\n',\n",
       " 'RESULTS\\tThere was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , PGA , and @MWD at @ weeks .\\n',\n",
       " 'RESULTS\\tThe mean difference between treatment arms ( @ % CI ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .\\n',\n",
       " 'RESULTS\\tFurther , there was a clinically relevant reduction in the serum levels of IL-@ , IL-@ , TNF - , and hsCRP at @ weeks in the intervention group when compared to the placebo group .\\n',\n",
       " 'RESULTS\\tThese differences remained significant at @ weeks .\\n',\n",
       " 'RESULTS\\tThe Outcome Measures in Rheumatology Clinical Trials-Osteoarthritis Research Society International responder rate was @ % in the intervention group and @ % in the placebo group ( p < @ ) .\\n',\n",
       " 'CONCLUSIONS\\tLow-dose oral prednisolone had both a short-term and a longer sustained effect resulting in less knee pain , better physical function , and attenuation of systemic inflammation in older patients with knee OA ( ClinicalTrials.gov identifier NCT@ ) .\\n',\n",
       " '\\n',\n",
       " '###24854809\\n',\n",
       " 'BACKGROUND\\tEmotional eating is associated with overeating and the development of obesity .\\n',\n",
       " 'BACKGROUND\\tYet , empirical evidence for individual ( trait ) differences in emotional eating and cognitive mechanisms that contribute to eating during sad mood remain equivocal .\\n',\n",
       " 'OBJECTIVE\\tThe aim of this study was to test if attention bias for food moderates the effect of self-reported emotional eating during sad mood ( vs neutral mood ) on actual food intake .\\n',\n",
       " 'OBJECTIVE\\tIt was expected that emotional eating is predictive of elevated attention for food and higher food intake after an experimentally induced sad mood and that attentional maintenance on food predicts food intake during a sad versus a neutral mood .\\n',\n",
       " 'METHODS\\tParticipants ( N = @ ) were randomly assigned to one of the two experimental mood induction conditions ( sad/neutral ) .\\n',\n",
       " 'METHODS\\tAttentional biases for high caloric foods were measured by eye tracking during a visual probe task with pictorial food and neutral stimuli .\\n',\n",
       " 'METHODS\\tSelf-reported emotional eating was assessed with the Dutch Eating Behavior Questionnaire ( DEBQ ) and ad libitum food intake was tested by a disguised food offer .\\n',\n",
       " 'RESULTS\\tHierarchical multivariate regression modeling showed that self-reported emotional eating did not account for changes in attention allocation for food or food intake in either condition .\\n',\n",
       " 'RESULTS\\tYet , attention maintenance on food cues was significantly related to increased intake specifically in the neutral condition , but not in the sad mood condition .\\n',\n",
       " 'CONCLUSIONS\\tThe current findings show that self-reported emotional eating ( based on the DEBQ ) might not validly predict who overeats when sad , at least not in a laboratory setting with healthy women .\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write a function to read in all the lines of a target text file\n",
    "def get_lines(filepath):\n",
    "    \"\"\"\n",
    "    Reads in a text file and returns a list of lines.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The path to the text file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of lines in the text file.\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\") as f:\n",
    "        return f.readlines()\n",
    "\n",
    "\n",
    "# Check the first 10 lines of the train file\n",
    "train_lines = get_lines(data_dir + \"train.txt\")\n",
    "train_lines[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structuring the data\n",
    "\n",
    "```\n",
    "[\n",
    "    {\n",
    "        'line_number': 0, \n",
    "        'target': 'BACKGROUND',\n",
    "        'text': 'Emotional eating is associated with overeating and the development of obesity .\\n',\n",
    "        'total_lines': 11, \n",
    "    }, \n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that takes a list of lines and returns a list of dictionarie\n",
    "def create_dicts(filepath):\n",
    "    \"\"\"\n",
    "    Creates a list of dictionaries of abstract line data\n",
    "\n",
    "    Args:\n",
    "        filepath.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries with the keys \"line_number\", \"target\", \"text\", \"total_lines\", abstract_id.\n",
    "    \"\"\"\n",
    "    abstract_lines = \"\" # Create an empty abstract\n",
    "    abstract_samples = [] # Create an empty list of abstract samples\n",
    "    input_lines = get_lines(filepath)\n",
    "\n",
    "    for i, line in enumerate(input_lines):\n",
    "        if line.startswith(\"###\"):\n",
    "            abstract_id = line\n",
    "            abstract_lines = \"\" # reset the abstract string if the line is an ID line\n",
    "        elif line.isspace(): \n",
    "            # if line is end of abstract, take abstract lines and create a dictionary, \n",
    "            # then append the dictionary to abstract_samples\n",
    "            abstract_line_split = abstract_lines.splitlines() # split abstract lines on new line\n",
    "            # Iterate through each line in a single abstract and keep count\n",
    "            for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
    "                line_split = abstract_line.split(\"\\t\")\n",
    "                # Create a dictionary of the line data\n",
    "                abstract_sample = {\n",
    "                    \"line_number\": abstract_line_number,\n",
    "                    \"target\": line_split[0],\n",
    "                    \"text\": line_split[1].lower(),\n",
    "                    \"total_lines\": len(abstract_line_split) - 1,\n",
    "                    \"abstract_id\": abstract_id\n",
    "                }\n",
    "                abstract_samples.append(abstract_sample)\n",
    "        else: # appends line to abstract lines if the end of the abstract is not reached\n",
    "            abstract_lines += line\n",
    "\n",
    "  \n",
    "    return abstract_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180040, 30212, 30135)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data from each file and preprocess it\n",
    "train_samples = create_dicts(data_dir + \"train.txt\")\n",
    "val_samples = create_dicts(data_dir + \"dev.txt\")\n",
    "test_samples = create_dicts(data_dir + \"test.txt\")\n",
    "len(train_samples),  len(val_samples), len(test_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_number</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>total_lines</th>\n",
       "      <th>abstract_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>to investigate the efficacy of @ weeks of dail...</td>\n",
       "      <td>11</td>\n",
       "      <td>###24293578\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>a total of @ patients with primary knee oa wer...</td>\n",
       "      <td>11</td>\n",
       "      <td>###24293578\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>outcome measures included pain reduction and i...</td>\n",
       "      <td>11</td>\n",
       "      <td>###24293578\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>pain was assessed using the visual analog pain...</td>\n",
       "      <td>11</td>\n",
       "      <td>###24293578\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>secondary outcome measures included the wester...</td>\n",
       "      <td>11</td>\n",
       "      <td>###24293578\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>serum levels of interleukin @ ( il-@ ) , il-@ ...</td>\n",
       "      <td>11</td>\n",
       "      <td>###24293578\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>there was a clinically relevant reduction in t...</td>\n",
       "      <td>11</td>\n",
       "      <td>###24293578\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>the mean difference between treatment arms ( @...</td>\n",
       "      <td>11</td>\n",
       "      <td>###24293578\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>further , there was a clinically relevant redu...</td>\n",
       "      <td>11</td>\n",
       "      <td>###24293578\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>these differences remained significant at @ we...</td>\n",
       "      <td>11</td>\n",
       "      <td>###24293578\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>the outcome measures in rheumatology clinical ...</td>\n",
       "      <td>11</td>\n",
       "      <td>###24293578\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>low-dose oral prednisolone had both a short-te...</td>\n",
       "      <td>11</td>\n",
       "      <td>###24293578\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>emotional eating is associated with overeating...</td>\n",
       "      <td>10</td>\n",
       "      <td>###24854809\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>yet , empirical evidence for individual ( trai...</td>\n",
       "      <td>10</td>\n",
       "      <td>###24854809\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    line_number       target  \\\n",
       "0             0    OBJECTIVE   \n",
       "1             1      METHODS   \n",
       "2             2      METHODS   \n",
       "3             3      METHODS   \n",
       "4             4      METHODS   \n",
       "5             5      METHODS   \n",
       "6             6      RESULTS   \n",
       "7             7      RESULTS   \n",
       "8             8      RESULTS   \n",
       "9             9      RESULTS   \n",
       "10           10      RESULTS   \n",
       "11           11  CONCLUSIONS   \n",
       "12            0   BACKGROUND   \n",
       "13            1   BACKGROUND   \n",
       "\n",
       "                                                 text  total_lines  \\\n",
       "0   to investigate the efficacy of @ weeks of dail...           11   \n",
       "1   a total of @ patients with primary knee oa wer...           11   \n",
       "2   outcome measures included pain reduction and i...           11   \n",
       "3   pain was assessed using the visual analog pain...           11   \n",
       "4   secondary outcome measures included the wester...           11   \n",
       "5   serum levels of interleukin @ ( il-@ ) , il-@ ...           11   \n",
       "6   there was a clinically relevant reduction in t...           11   \n",
       "7   the mean difference between treatment arms ( @...           11   \n",
       "8   further , there was a clinically relevant redu...           11   \n",
       "9   these differences remained significant at @ we...           11   \n",
       "10  the outcome measures in rheumatology clinical ...           11   \n",
       "11  low-dose oral prednisolone had both a short-te...           11   \n",
       "12  emotional eating is associated with overeating...           10   \n",
       "13  yet , empirical evidence for individual ( trai...           10   \n",
       "\n",
       "      abstract_id  \n",
       "0   ###24293578\\n  \n",
       "1   ###24293578\\n  \n",
       "2   ###24293578\\n  \n",
       "3   ###24293578\\n  \n",
       "4   ###24293578\\n  \n",
       "5   ###24293578\\n  \n",
       "6   ###24293578\\n  \n",
       "7   ###24293578\\n  \n",
       "8   ###24293578\\n  \n",
       "9   ###24293578\\n  \n",
       "10  ###24293578\\n  \n",
       "11  ###24293578\\n  \n",
       "12  ###24854809\\n  \n",
       "13  ###24854809\\n  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.DataFrame(train_samples)\n",
    "val_df = pd.DataFrame(val_samples)\n",
    "test_df = pd.DataFrame(test_samples)\n",
    "train_df.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "METHODS        59353\n",
       "RESULTS        57953\n",
       "CONCLUSIONS    27168\n",
       "BACKGROUND     21727\n",
       "OBJECTIVE      13839\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of labels in training data\n",
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Distribution of total lines in abstracts')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check length of different lines\n",
    "train_df.total_lines.plot.hist()\n",
    "plt.title(\"Distribution of total lines in abstracts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get lists of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180040, 30212, 30135)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert abstract text lines into lists\n",
    "train_sentences = train_df.text.tolist()\n",
    "val_sentences = val_df.text.tolist()\n",
    "test_sentences = test_df.text.tolist()\n",
    "len(train_sentences), len(val_sentences), len(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n",
       " 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n",
       " 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',\n",
       " 'pain was assessed using the visual analog pain scale ( @-@ mm ) .',\n",
       " 'secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .',\n",
       " 'serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .',\n",
       " 'there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .',\n",
       " 'the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .',\n",
       " 'further , there was a clinically relevant reduction in the serum levels of il-@ , il-@ , tnf - , and hscrp at @ weeks in the intervention group when compared to the placebo group .',\n",
       " 'these differences remained significant at @ weeks .']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first 10 lines of the training data\n",
    "train_sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make numeric labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 11:41:54.174888: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(180040, 5), dtype=float64, numpy=\n",
       "array([[0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Onehot encoded labels\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "# alternatively use tf.one_hot\n",
    "one_hot_encoder = OneHotEncoder(sparse=False) # We want a non-sparse matrix\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_df.target.to_numpy().reshape(-1, 1))\n",
    "val_labels_one_hot = one_hot_encoder.transform(val_df.target.to_numpy().reshape(-1, 1))\n",
    "test_labels_one_hot = one_hot_encoder.transform(test_df.target.to_numpy().reshape(-1, 1))\n",
    "# Check what one_hot encoded labels look like\n",
    "train_labels_one_hot\n",
    "tf.constant(train_labels_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract labels (\"target\" columns) and encode them into integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_df.target.to_numpy())\n",
    "val_labels_encoded = label_encoder.transform(val_df.target.to_numpy())\n",
    "test_labels_encoded = label_encoder.transform(test_df.target.to_numpy())\n",
    "# Check what encoded labels look like\n",
    "train_labels_encoded[:12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get class names and number of classes from LabelEncoder instance\n",
    "num_classes = len(label_encoder.classes_)\n",
    "class_names = label_encoder.classes_\n",
    "num_classes, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series of Experiments\n",
    "\n",
    "* 0 - Naive Bayes with TF-IDF encoder (baseline)\n",
    "* 1 - Conv1D with token embeddings\n",
    "* 2 - TF Hub Pretrained Feature Extractor\n",
    "* 3 - Conv1D with character embeddings\n",
    "* 4 - Combining pretrained token embeddings + characters embeddings (hybrid embedding layer)\n",
    "* 5 - Combining pretrained token embeddings + characters embeddings + positional embeddings\n",
    "\n",
    "[Machine Learning Testing Map](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Baseline Model: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create tokenization and modelling pipeline\n",
    "model_0 = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n",
    "    (\"clf\", MultinomialNB()), # model the text using a naive bayes classifier\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(train_sentences, train_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy score: 72.18%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our baseline model\n",
    "baseline_score =  model_0.score(val_sentences, val_labels_encoded)\n",
    "print(f'Baseline accuracy score: {baseline_score * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 3, ..., 4, 4, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions using baseline model\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use helper functions script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 72.1832384482987,\n",
       " 'precision': 0.7186466952323352,\n",
       " 'recall': 0.7218323844829869,\n",
       " 'f1': 0.6989250353450294}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from _helper_functions import calculate_results\n",
    "# Calculate baseline results\n",
    "baseline_results = calculate_results(y_true=val_labels_encoded,\n",
    "                                        y_pred=baseline_preds)\n",
    "\n",
    "baseline_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.338269273494777\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Find the average number of tokens (words) in the training sentences\n",
    "sent_lens = [len(sentence.split()) for sentence in train_sentences]\n",
    "avg_sent_len = np.mean(sent_lens)\n",
    "print(avg_sent_len)\n",
    "\n",
    "# What's the distribution look like?\n",
    "plt.hist(sent_lens, bins=20)\n",
    "plt.title(\"Distribution of sentence length\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 tokens (words) or less covers 95% of training examples\n"
     ]
    }
   ],
   "source": [
    "# How long of a sentence length covers 95% of the examples?\n",
    "output_seq_len = int(np.percentile(sent_lens, 95))\n",
    "print(output_seq_len, \"tokens (words) or less covers 95% of training examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize train sentences, turn it into an embedding and build a model\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import random\n",
    "\n",
    "# Setup text vectorization variables\n",
    "max_vocab_len = 68000 # 68k value used in paper for PubMed 20k RCT, 200k is 331k\n",
    "\n",
    "# Create text vectorizer\n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab_len,\n",
    "                                    output_sequence_length=output_seq_len)\n",
    "                                    \n",
    "# Adapt the text vectorizer to the training texts\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "for routine use , ons with enema instead of traditional preparation for colonoscopy with peg can not be generally recommended .\n",
      "\n",
      "Length of text: 21\n",
      "\n",
      "Vectorized text: [[   11   633    87  8627     7 12688  4236     4   803  1406    11  1629\n",
      "      7  3234   171    31    36  1153   730     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# Vectorize random sentence & view\n",
    "target_sentence = random.choice(train_sentences)\n",
    "vectorized_sentence = text_vectorizer([target_sentence])\n",
    "print(f\"Text:\\n{target_sentence}\")\n",
    "print(f\"\\nLength of text: {len(target_sentence.split())}\")\n",
    "print(f\"\\nVectorized text: {vectorized_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 64841\n",
      "Most common words: ['', '[UNK]', 'the', 'and', 'of']\n",
      "Least common words: ['aainduced', 'aaigroup', 'aachener', 'aachen', 'aaacp']\n"
     ]
    }
   ],
   "source": [
    "# Explore: How many words are in the vocabulary?\n",
    "rct_20k_text_vocab = text_vectorizer.get_vocabulary()\n",
    "print(f\"Number of words in vocab: {len(rct_20k_text_vocab)}\")\n",
    "print(f\"Most common words: {rct_20k_text_vocab[:5]}\")\n",
    "print(f\"Least common words: {rct_20k_text_vocab[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'text_vectorization_2',\n",
       " 'trainable': True,\n",
       " 'batch_input_shape': (None,),\n",
       " 'dtype': 'string',\n",
       " 'max_tokens': 68000,\n",
       " 'standardize': 'lower_and_strip_punctuation',\n",
       " 'split': 'whitespace',\n",
       " 'ngrams': None,\n",
       " 'output_mode': 'int',\n",
       " 'output_sequence_length': 55,\n",
       " 'pad_to_max_tokens': False,\n",
       " 'sparse': False,\n",
       " 'ragged': False,\n",
       " 'vocabulary': None,\n",
       " 'idf_weights': None}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the config of our text vectorizer\n",
    "text_vectorizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "# Create an embedding layer\n",
    "token_embed = layers.Embedding(input_dim=len(rct_20k_text_vocab), # length of vocab\n",
    "                                output_dim=128, # size of embedding vectors\n",
    "                                mask_zero=True, # whether or not the input value 0 is a special \"padding\" value which should be masked out\n",
    "                                name=\"token_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " for routine use , ons with enema instead of traditional preparation for colonoscopy with peg can not be generally recommended .        \n",
      "\n",
      "Vectorized version: [[   11   633    87  8627     7 12688  4236     4   803  1406    11  1629\n",
      "      7  3234   171    31    36  1153   730     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0]]\n",
      "\n",
      "Embedded version: [[[-2.2568703e-02 -4.4393148e-02 -2.9465668e-03 ... -2.3519194e-02\n",
      "   -3.3806909e-02 -2.1203041e-02]\n",
      "  [-2.3031607e-03 -3.9004795e-03  8.9123473e-03 ... -1.5198216e-03\n",
      "    4.0409457e-02 -1.1797033e-02]\n",
      "  [-2.9892052e-02 -3.5852589e-02  1.9468654e-02 ... -3.9160110e-02\n",
      "   -7.9143047e-04  2.8103758e-02]\n",
      "  ...\n",
      "  [-4.6045531e-02  2.1658838e-05  2.7361598e-02 ... -3.8099527e-02\n",
      "    3.4526791e-02  3.3373151e-02]\n",
      "  [-4.6045531e-02  2.1658838e-05  2.7361598e-02 ... -3.8099527e-02\n",
      "    3.4526791e-02  3.3373151e-02]\n",
      "  [-4.6045531e-02  2.1658838e-05  2.7361598e-02 ... -3.8099527e-02\n",
      "    3.4526791e-02  3.3373151e-02]]]\n",
      "\n",
      "Embedded version shape: (1, 55, 128)\n"
     ]
    }
   ],
   "source": [
    "# Show example embedding\n",
    "\n",
    "print(f\"Original text:\\n {target_sentence}\\\n",
    "        \\n\\nVectorized version: {vectorized_sentence}\")  \n",
    "embedded_sentence = token_embed(vectorized_sentence)\n",
    "print(f\"\\nEmbedded version: {embedded_sentence}\")\n",
    "print(f\"\\nEmbedded version shape: {embedded_sentence.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(5,), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create datasets with the tf.data API\n",
    "# Turn the data into TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot))\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the TensorFlow datasets and turn them into prefetched batches\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_conv1D\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_2 (TextV  (None, 55)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " token_embedding (Embedding)  (None, 55, 128)          8299648   \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 51, 64)            41024     \n",
      "                                                                 \n",
      " global_average_pooling1d_4   (None, 64)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,340,997\n",
      "Trainable params: 8,340,997\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a 1D conv model with token embedding to process sequences\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "text_vectors = text_vectorizer(inputs)\n",
    "token_embeddings = token_embed(text_vectors)\n",
    "x = layers.Conv1D(64, 5, activation=\"relu\")(token_embeddings)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_conv1D\")\n",
    "\n",
    "# Get a summary of our model\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _helper_functions import create_tensorboard_callback\n",
    "#Compile model\n",
    "model_1.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: tensorflow_hub/model_1_conv1D/20220914\n",
      "Epoch 1/3\n",
      "562/562 [==============================] - 29s 51ms/step - loss: 0.9517 - accuracy: 0.6209 - val_loss: 0.7243 - val_accuracy: 0.7261\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 27s 47ms/step - loss: 0.6901 - accuracy: 0.7427 - val_loss: 0.6656 - val_accuracy: 0.7590\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 26s 46ms/step - loss: 0.6454 - accuracy: 0.7638 - val_loss: 0.6230 - val_accuracy: 0.7759\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history_1 = model_1.fit(train_dataset,\n",
    "                        steps_per_epoch=int(0.1*len(train_dataset)),\n",
    "                        epochs=3,\n",
    "                        validation_data=val_dataset,\n",
    "                        validation_steps=int(0.1*len(val_dataset)), # only validates on 10% of validation data\n",
    "                        callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\",\n",
    "                                                                experiment_name=\"model_1_conv1D\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 2s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "model_1_pred_probs = model_1.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[4.33648944e-01, 1.96612448e-01, 7.57519603e-02, 2.64770389e-01,\n",
       "         2.92161107e-02],\n",
       "        [4.89116788e-01, 1.99094385e-01, 1.26281846e-02, 2.91080475e-01,\n",
       "         8.08016490e-03],\n",
       "        [1.98368773e-01, 2.71563586e-02, 3.71783995e-03, 7.70604968e-01,\n",
       "         1.52008986e-04],\n",
       "        ...,\n",
       "        [3.74743240e-06, 9.26175271e-04, 5.91134769e-04, 3.23135487e-06,\n",
       "         9.98475730e-01],\n",
       "        [6.07970282e-02, 3.80269825e-01, 1.60065830e-01, 9.51323286e-02,\n",
       "         3.03734958e-01],\n",
       "        [2.20162168e-01, 5.31385243e-01, 7.52954185e-02, 6.82504624e-02,\n",
       "         1.04906775e-01]], dtype=float32),\n",
       " (30212, 5))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_pred_probs, model_1_pred_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int64, numpy=array([0, 0, 3, 2, 4, 2, 4, 2, 4, 1])>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert prediction probabilities to class labels\n",
    "model_1_preds = tf.argmax(model_1_pred_probs, axis=1)\n",
    "model_1_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 77.80021183635641,\n",
       " 'precision': 0.7736911544544379,\n",
       " 'recall': 0.7780021183635641,\n",
       " 'f1': 0.7749848659001156}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model results\n",
    "model_1_results = calculate_results(y_true=val_labels_encoded,\n",
    "                                    y_pred=model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 72.1832384482987,\n",
       " 'precision': 0.7186466952323352,\n",
       " 'recall': 0.7218323844829869,\n",
       " 'f1': 0.6989250353450294}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eef4eb45b5ce6f96548309064ae87146eaf6cbe4b05ab916c771d60a96931cae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
