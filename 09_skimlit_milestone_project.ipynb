{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone Project 2: SkimLit\n",
    "\n",
    "Sequence Problem:\n",
    "Many to One Classification\n",
    "\n",
    "* Download PubMed 200k RCT dataset\n",
    "* Preprocess the text data\n",
    "* Set up multiple modeling experiments\n",
    "* Build a multimodal model to take in different sources of data\n",
    "  * Replicate the model powering https://arxiv.org/abs/1710.06071\n",
    "* Find the most wrong prediction examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skim Medical Literature \n",
    "\n",
    "A Dataset for sequential Sentence Classification in Medical Abastracts:\n",
    "\n",
    "[Source](https://arxiv.org/abs/1710.06071)\n",
    "\n",
    "[Model Architecture](https://arxiv.org/abs/1612.05251)\n",
    "\n",
    "Artificial Neural Network consisting of 3 main components:\n",
    "* Token embedding layer (bi-LSTM)\n",
    "* Sentence label prediction layer (bi-LSTM)\n",
    "* Label sequence optimization layer (CRF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm access to GPU\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset\n",
    "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git ../Downloads/09_skimlit_milestone_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check downloaded folders\n",
    "!ls ../Downloads/09_skimlit_milestone_project\n",
    "\n",
    "# Check files in the one of the folders\n",
    "!ls ../Downloads/09_skimlit_milestone_project/PubMed_20k_RCT_numbers_replaced_with_at_sign/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 20k RCT dataset with numbers replaced with @ sign\n",
    "data_dir = \"../Downloads/09_skimlit_milestone_project/PubMed_20k_RCT_numbers_replaced_with_at_sign/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the filenames in target dir\n",
    "import os\n",
    "filenames = os.listdir(data_dir)\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to read in all the lines of a target text file\n",
    "def get_lines(filepath):\n",
    "    \"\"\"\n",
    "    Reads in a text file and returns a list of lines.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The path to the text file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of lines in the text file.\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\") as f:\n",
    "        return f.readlines()\n",
    "\n",
    "\n",
    "# Check the first 10 lines of the train file\n",
    "train_lines = get_lines(data_dir + \"train.txt\")\n",
    "train_lines[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structuring the data\n",
    "\n",
    "```\n",
    "[\n",
    "    {\n",
    "        'line_number': 0, \n",
    "        'target': 'BACKGROUND',\n",
    "        'text': 'Emotional eating is associated with overeating and the development of obesity .\\n',\n",
    "        'total_lines': 11, \n",
    "    }, \n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that takes a list of lines and returns a list of dictionarie\n",
    "def create_dicts(filepath):\n",
    "    \"\"\"\n",
    "    Creates a list of dictionaries of abstract line data\n",
    "\n",
    "    Args:\n",
    "        filepath.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries with the keys \"line_number\", \"target\", \"text\", \"total_lines\", abstract_id.\n",
    "    \"\"\"\n",
    "    abstract_lines = \"\" # Create an empty abstract\n",
    "    abstract_samples = [] # Create an empty list of abstract samples\n",
    "    input_lines = get_lines(filepath)\n",
    "\n",
    "    for i, line in enumerate(input_lines):\n",
    "        if line.startswith(\"###\"):\n",
    "            abstract_id = line\n",
    "            abstract_lines = \"\" # reset the abstract string if the line is an ID line\n",
    "        elif line.isspace(): \n",
    "            # if line is end of abstract, take abstract lines and create a dictionary, \n",
    "            # then append the dictionary to abstract_samples\n",
    "            abstract_line_split = abstract_lines.splitlines() # split abstract lines on new line\n",
    "            # Iterate through each line in a single abstract and keep count\n",
    "            for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
    "                line_split = abstract_line.split(\"\\t\")\n",
    "                # Create a dictionary of the line data\n",
    "                abstract_sample = {\n",
    "                    \"line_number\": abstract_line_number,\n",
    "                    \"target\": line_split[0],\n",
    "                    \"text\": line_split[1].lower(),\n",
    "                    \"total_lines\": len(abstract_line_split) - 1,\n",
    "                    \"abstract_id\": abstract_id\n",
    "                }\n",
    "                abstract_samples.append(abstract_sample)\n",
    "        else: # appends line to abstract lines if the end of the abstract is not reached\n",
    "            abstract_lines += line\n",
    "\n",
    "  \n",
    "    return abstract_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from each file and preprocess it\n",
    "train_samples = create_dicts(data_dir + \"train.txt\")\n",
    "val_samples = create_dicts(data_dir + \"dev.txt\")\n",
    "test_samples = create_dicts(data_dir + \"test.txt\")\n",
    "len(train_samples),  len(val_samples), len(test_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.DataFrame(train_samples)\n",
    "val_df = pd.DataFrame(val_samples)\n",
    "test_df = pd.DataFrame(test_samples)\n",
    "train_df.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of labels in training data\n",
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check length of different lines\n",
    "train_df.total_lines.plot.hist()\n",
    "plt.title(\"Distribution of total lines in abstracts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get lists of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert abstract text lines into lists\n",
    "train_sentences = train_df.text.tolist()\n",
    "val_sentences = val_df.text.tolist()\n",
    "test_sentences = test_df.text.tolist()\n",
    "len(train_sentences), len(val_sentences), len(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 10 lines of the training data\n",
    "train_sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make numeric labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Onehot encoded labels\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "# alternatively use tf.one_hot\n",
    "one_hot_encoder = OneHotEncoder(sparse=False) # We want a non-sparse matrix\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_df.target.to_numpy().reshape(-1, 1))\n",
    "val_labels_one_hot = one_hot_encoder.transform(val_df.target.to_numpy().reshape(-1, 1))\n",
    "test_labels_one_hot = one_hot_encoder.transform(test_df.target.to_numpy().reshape(-1, 1))\n",
    "# Check what one_hot encoded labels look like\n",
    "train_labels_one_hot\n",
    "tf.constant(train_labels_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels (\"target\" columns) and encode them into integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_df.target.to_numpy())\n",
    "val_labels_encoded = label_encoder.transform(val_df.target.to_numpy())\n",
    "test_labels_encoded = label_encoder.transform(test_df.target.to_numpy())\n",
    "# Check what encoded labels look like\n",
    "train_labels_encoded[:12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class names and number of classes from LabelEncoder instance\n",
    "num_classes = len(label_encoder.classes_)\n",
    "class_names = label_encoder.classes_\n",
    "num_classes, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series of Experiments\n",
    "\n",
    "* 0 - Naive Bayes with TF-IDF encoder (baseline)\n",
    "* 1 - Conv1D with token embeddings\n",
    "* 2 - TF Hub Pretrained Feature Extractor\n",
    "* 3 - Conv1D with character embeddings\n",
    "* 4 - Combining pretrained token embeddings + characters embeddings (hybrid embedding layer)\n",
    "* 5 - Combining pretrained token embeddings + characters embeddings + positional embeddings\n",
    "\n",
    "[Machine Learning Testing Map](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Baseline Model: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create tokenization and modelling pipeline\n",
    "model_0 = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n",
    "    (\"clf\", MultinomialNB()), # model the text using a naive bayes classifier\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(train_sentences, train_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate our baseline model\n",
    "baseline_score =  model_0.score(val_sentences, val_labels_encoded)\n",
    "print(f'Baseline accuracy score: {baseline_score * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using baseline model\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use helper functions script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _helper_functions import calculate_results\n",
    "# Calculate baseline results\n",
    "baseline_results = calculate_results(y_true=val_labels_encoded,\n",
    "                                        y_pred=baseline_preds)\n",
    "\n",
    "baseline_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Find the average number of tokens (words) in the training sentences\n",
    "sent_lens = [len(sentence.split()) for sentence in train_sentences]\n",
    "avg_sent_len = np.mean(sent_lens)\n",
    "print(avg_sent_len)\n",
    "\n",
    "# What's the distribution look like?\n",
    "plt.hist(sent_lens, bins=20)\n",
    "plt.title(\"Distribution of sentence length\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How long of a sentence length covers 95% of the examples?\n",
    "output_seq_len = int(np.percentile(sent_lens, 95))\n",
    "print(output_seq_len, \"tokens (words) or less covers 95% of training examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize train sentences, turn it into an embedding and build a model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import random\n",
    "\n",
    "# Setup text vectorization variables\n",
    "max_vocab_length = 10000\n",
    "max_length = 15\n",
    "\n",
    "# Create text vectorizer\n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
    "                                    output_mode=\"int\",\n",
    "                                    output_sequence_length=max_length)\n",
    "\n",
    "# Fit the text vectorizer to the training text\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "# Create a sample sentence and tokenize it\n",
    "sample_sentence = \"There are two types of tensors: scalars (0D tensors), vectors (1D tensors), matrices (2D tensors), and tensors with more axes (3D tensors and higher).\"\n",
    "text_vectorizer([sample_sentence])\n",
    "\n",
    "# Get the unique words in the vocabulary\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "top_5_words = words_in_vocab[:5]\n",
    "bottom_5_words = words_in_vocab[-5:]\n",
    "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
    "print(f\"5 random words from vocab: {top_5_words}\")\n",
    "print(f\"5 random words from vocab: {bottom_5_words}\")\n",
    "\n",
    "# Create an embedding layer\n",
    "embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                output_dim=128,\n",
    "                                embeddings_initializer=\"uniform\",\n",
    "                                input_length=max_length)\n",
    "\n",
    "# Get a random sentence from the training set\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n {random_sentence}\\\n",
    "        \\n\\nVectorized version:\")   \n",
    "\n",
    "# Embed the random sentence (you can also pass a list of sentences)\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embed\n",
    "\n",
    "# Create a 1D conv model to process sequences\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.Conv1D(64, 5, activation=\"relu\")(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_conv1D\")\n",
    "\n",
    "# Get a summary of our model\n",
    "model_1.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _helper_functions import create_tensorboard_callback\n",
    "#Compile model\n",
    "model_1.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit the model\n",
    "history_1 = model_1.fit(train_sentences,    \n",
    "                        train_labels_one_hot,\n",
    "                        epochs=5,\n",
    "                        validation_data=(val_sentences, val_labels_one_hot),\n",
    "                        callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\",\n",
    "                                                                experiment_name=\"model_1_conv1D\")])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eef4eb45b5ce6f96548309064ae87146eaf6cbe4b05ab916c771d60a96931cae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
