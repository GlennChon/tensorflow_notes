{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning with TensorFlow Part 3\n",
    "\n",
    "Scaling Up (Food Vision mini)\n",
    "101 total food classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check gpu\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, compare_historys, walk_through_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Food 101\n",
    "\n",
    "Goal: Beat the original Food101 paper with 10% of the training data\n",
    "### Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "!wget -nc -P ../Downloads/ https://storage.googleapis.com/ztm_tf_course/food_vision/101_food_classes_10_percent.zip\n",
    "\n",
    "# Unzip\n",
    "unzip_data('../Downloads/101_food_classes_10_percent.zip', '../Downloads')\n",
    "\n",
    "# Check number of images and subdirectories in the dataset\n",
    "walk_through_dir('../Downloads/101_food_classes_10_percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '../Downloads/101_food_classes_10_percent/train'\n",
    "test_dir = '../Downloads/101_food_classes_10_percent/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data inputs\n",
    "import tensorflow as tf\n",
    "IMG_SIZE = (224, 224)\n",
    "train_data_all_10_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir,\n",
    "                                                                                label_mode=\"categorical\",\n",
    "                                                                                image_size=IMG_SIZE)\n",
    "                                                                                \n",
    "test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n",
    "                                                                label_mode=\"categorical\",\n",
    "                                                                image_size=IMG_SIZE,\n",
    "                                                                shuffle=False) # don't shuffle test data for prediction analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the big model with transfer learning\n",
    "\n",
    "10% of 101 food classes\n",
    "\n",
    "Steps:\n",
    "1. Create a ModelCheckpoint callback\n",
    "2. Create a data agumentation layer to build data agumentation right into the model\n",
    "3. Build a headless (no top layers) functional EfficientNetB0 base model\n",
    "    a. Create a custom output layer\n",
    "4. Compile\n",
    "5. Feature extract for 5 full passes \n",
    "    a. 5 epochs on the train dataset\n",
    "    b. Validate on 15% of the test data (to save time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '../checkpoints/101_classes_10_percent_data_model_checkpoint'\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                        save_weights_only=True,\n",
    "                                                        monitor=\"val_accuracy\",\n",
    "                                                        save_best_only=True,\n",
    "                                                        save_freq=\"epoch\", #default is save every epoch,\n",
    "                                                        verbose=1\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import print_random_img\n",
    "\n",
    "print_random_img(test_dir, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data aug layer\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Setup data aug\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    data_aug = Sequential([\n",
    "        preprocessing.RandomFlip('horizontal'),\n",
    "        preprocessing.RandomRotation(0.2),\n",
    "        preprocessing.RandomHeight(0.2),\n",
    "        preprocessing.RandomWidth(0.2),\n",
    "        preprocessing.RandomZoom(0.2),\n",
    "    #     preprocessing.Rescaling(1/255.), # rescale inputs of images to between 0 & 1. Required for models like resnet50 \n",
    "    ], name=\"data_aug\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup base model and freeze\n",
    "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
    "base_model.trainable = False\n",
    "\n",
    "\n",
    "# Setup model architecture with trainable top layers\n",
    "inputs = layers.Input(shape=(224, 224, 3), name=\"input_layer\")\n",
    "x = data_aug(inputs) # augment images (only during training)\n",
    "x = base_model(x, training=False) # put the  base model in inference mode so weights are not updated\n",
    "x = layers.GlobalAveragePooling2D(name=\"global_avg_pool_layer\")(x)\n",
    "outputs = layers.Dense(len(train_data_all_10_percent.class_names), activation=\"softmax\", name=\"output_layer\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), \n",
    "            optimizer=tf.keras.optimizers.Adam(),\n",
    "            metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit\n",
    "history_all_classes_10_percent = model.fit(train_data_all_10_percent,\n",
    "                                           epochs=5, # fit for 5 epochs to keep experiments quick\n",
    "                                           validation_data=test_data,\n",
    "                                           validation_steps=int(0.15 * len(test_data)), # evaluate on smaller portion of test data\n",
    "                                           callbacks=[checkpoint_callback]) # save best model weights to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_results = model.evaluate(test_data)\n",
    "fine_tuning_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(history_all_classes_10_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine Tune\n",
    "# Unfreeze all the layers in the base_model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Refreeze every layer except top 5 layers\n",
    "for layer in base_model.layers[:-5]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recompile model, lower learning rate by 10x\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print layer name and index if layer is trainable\n",
    "for layer_number, layer in enumerate(model.layers[2].layers):\n",
    "    if layer.trainable:\n",
    "        print(f'Index: {layer_number}', f'Name: {layer.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fine_tune_epochs = 10\n",
    "\n",
    "#fine-tune our model\n",
    "history_all_classes_10_percent_fine_tune = model.fit(train_data_all_10_percent,\n",
    "          epochs=fine_tune_epochs, # fit for 5 epochs to keep experiments quick\n",
    "          validation_data=test_data,\n",
    "          validation_steps=int(0.15 * len(test_data)), # evaluate on smaller portion of test data\n",
    "          initial_epoch=history_all_classes_10_percent.epoch[-1], # start from the last epoch of the previous training\n",
    "          ) # save best model weights to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extraction_results = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_historys(original_history=history_all_classes_10_percent, \n",
    "                new_history=history_all_classes_10_percent_fine_tune,\n",
    "                initial_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "model.save(\"../models/101_food_classes_10_percent_data_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and evaluate\n",
    "loaded_model = tf.keras.models.load_model(\"../models/101_food_classes_10_percent_data_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_results = loaded_model.evaluate(test_data)\n",
    "loaded_model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the performance across all different classes\n",
    "\n",
    "1. Make predictions\n",
    "2. Visualize\n",
    "3. Find which predictions were the \"most\" wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with model\n",
    "preds_probs = model.predict(test_data, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many predictions\n",
    "len(preds_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many class predictions per image?\n",
    "# What's the shape of our predictions\n",
    "preds_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check first 10 predictions\n",
    "preds_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check first prediction probability array\n",
    "preds_probs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model outputs a prediction probability array (with N number of variables, \n",
    "# where N is the number of classes) for each sample passed to the predict method. \n",
    "# The highest probability is the predicted class.\n",
    "print(f'Number of prediction probabilities for sample 0: {len(preds_probs[0])}')\n",
    "print(f'Prediction probabilities for sample 0: {preds_probs[0]}')\n",
    "print(f'The class with the highest predicted probability by the model for sample 0: {preds_probs[0].argmax()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Highest prediction probability classname for sample 0: {test_data.class_names[preds_probs[0].argmax()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted classes of each label\n",
    "pred_classes = preds_probs.argmax(axis=1)\n",
    "pred_classes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To evaluate, we need to compare to the orginal test dataset labels\n",
    "# need to unbatch test_data\n",
    "y_labels = []\n",
    "for images, labels in test_data.unbatch():\n",
    "    y_labels.append(labels.numpy().argmax())\n",
    "    \n",
    "y_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many y_labels are there?\n",
    "len(y_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our model's predictions\n",
    "\n",
    "One way to check the model's predictions array is in the same order as our test labels array is to check the accuracy score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scikit-learn to compare the predicted classes to the original labels\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "sklearn_accuracy = accuracy_score(y_true=y_labels, y_pred=pred_classes)\n",
    "\n",
    "# Does this metric come close to our model's evaluate results?\n",
    "np.isclose(loaded_model_results[1], sklearn_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "import helper_functions as hf\n",
    "import importlib\n",
    "importlib.reload(hf)\n",
    "\n",
    "hf.make_confusion_matrix(y_true=y_labels, \n",
    "                        y_pred=pred_classes, \n",
    "                        classes=test_data.class_names, \n",
    "                        figsize=(100,100), \n",
    "                        text_size=30,\n",
    "                        savefig=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report\n",
    "\n",
    "`classification_report`: Scikit-learn's helpful function for acquiring many different classification metrics per class. (e.g. precision, recall, f1 score, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\n",
    "    classification_report(y_true=y_labels, \n",
    "                            y_pred=pred_classes, \n",
    "                            target_names=test_data.class_names\n",
    "                        )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dictionary of the classification\n",
    "classification_report_dict = classification_report(y_true=y_labels,\n",
    "                            y_pred=pred_classes,\n",
    "                            output_dict=True,\n",
    "                            target_names=test_data.class_names\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dictionary \n",
    "class_f1_scores = {}\n",
    "# Loop through each dictionary item and assign name to f1-score in separate dictionary\n",
    "for k, v in classification_report_dict.items():\n",
    "    if k == 'accuracy': # stop loop on 'accuracy'\n",
    "        break;\n",
    "    else:\n",
    "        # Add class names and f1-scores to new dictionary\n",
    "        class_f1_scores[k] = v[\"f1-score\"]\n",
    "\n",
    "class_f1_scores\n",
    "# classification_report_dict[\"bibimbap\"][\"f1-score\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "f1_scores = pd.DataFrame({\"class_names\":list(class_f1_scores.keys()),\n",
    "                         \"f1-score\":list(class_f1_scores.values())}).sort_values(\"f1-score\", ascending=False)\n",
    "\n",
    "f1_scores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot on bar chart\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 25))\n",
    "scores = ax.barh(range(len(f1_scores)), f1_scores[\"f1-score\"].values) # get f1-score values\n",
    "ax.set_yticks(range(len(f1_scores))) # set y-axis ticks to match f1-score values\n",
    "ax.set_yticklabels(f1_scores[\"class_names\"]) # set y-axis tick labels to match class names\n",
    "ax.set_xlabel(\"F1-score\")\n",
    "ax.set_title(\"F1-score for 101 Food Classes\")\n",
    "ax.invert_yaxis(); # reverse order\n",
    "\n",
    "ax.bar_label(scores, fmt='%.2f')\n",
    "ax.set_xlim(right=1.05)  # adjust xlim to fit labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Predictions on Custom Images\n",
    "\n",
    "Visualize our model's predictions on our own images\n",
    "Create a function to:\n",
    "1. Load an image\n",
    "2. Resize it to the appropriate size\n",
    "3. Preprocess it\n",
    "4. Predict the image\n",
    "5. Visualize the image and the prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to load and prepare images\n",
    "def load_and_prep_images(filename, img_shape=224, scale=True):\n",
    "    \"\"\"\n",
    "    Reads in an image from filename, turns it into a tensor and reshapes into specified shape \n",
    "    (img_shape, img_shape, color_channels=3).\n",
    "\n",
    "    Args:\n",
    "        filename (str): path to target image\n",
    "        image_shape (int): height/width dimension of target image size\n",
    "        scale (bool): scale pixel values from 0-255 to 0-1 or not\n",
    "\n",
    "    Returns:\n",
    "        Image tensor of shape (img_shape, img_shape, 3)\n",
    "    \"\"\"\n",
    "\n",
    "    # Read in image\n",
    "    img = tf.io.read_file(filename)\n",
    "    # Decode image into a tensor\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    # Resize image to specified shape\n",
    "    img = tf.image.resize(img, [img_shape, img_shape])\n",
    "    # Scale pixel values from 0-255 to 0-1\n",
    "    if scale:\n",
    "        img = img / 255.0\n",
    "    # Return tensor\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps\n",
    "\n",
    "1. Load a few random images\n",
    "2. Make predictions on loaded images\n",
    "3. Plot the original image(s) along with the model's predictions, prediction probability and truth label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "plt.figure(figsize=(17, 10))\n",
    "for i in range(3):\n",
    "    class_names = list(class_f1_scores.keys())\n",
    "    # Choose random image(s) from random class(es)\n",
    "    class_name = random.choice(class_names)\n",
    "    filename = random.choice(os.listdir(os.path.join(test_dir, class_name)))\n",
    "    filepath = os.path.join(test_dir, class_name, filename)\n",
    "    \n",
    "    # Load and prepare image\n",
    "    img = load_and_prep_images(filepath, scale=False)\n",
    "    img_expanded = tf.expand_dims(img, axis=0)\n",
    "    pred_prob = model.predict(img_expanded) # get prediction probability\n",
    "    pred_prob_max = pred_prob.argmax()\n",
    "    pred_class = class_names[pred_prob_max] # get highest prediction probability index\n",
    "\n",
    "    # Plot image(s)\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.imshow(img/255.)\n",
    "\n",
    "    if class_name == pred_class: # if predicted class matches truth class, make text green, else red\n",
    "        title_color = \"g\"\n",
    "    else:\n",
    "        title_color = \"r\"\n",
    "    plt.title(f\"Actual: {class_name} | Pred: {pred_class} | Prob: {pred_prob.max():.2f}\", color=title_color)\n",
    "    plt.axis(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the most incorrect predictions\n",
    "\n",
    "To find out where our model is the most incorrect:\n",
    "1. Get all of the image file paths in the test dataset using `list_files()` method\n",
    "2. Create a pandas DataFrame of the image filepaths, ground truth labels, predicted classes (from our model), max prediction probabilities, prediction class names, ground truth class names.\n",
    "3. Use the DataFrame to find all the wrong predictions (where the ground truth label doesn't match the prediction).\n",
    "4. Sort the DataFrame based on wrong predictions (descending).\n",
    "5. Visualize the images with the highest prediction probabilities but have the wrong prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get all the img filepaths\n",
    "import tensorflow as tf\n",
    "\n",
    "filepaths = []\n",
    "for filepath in test_data.list_files(os.path.join(test_dir, \"*/*.jpg\"), shuffle=False):\n",
    "    filepaths.append(filepath.numpy())\n",
    "\n",
    "filepaths[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a DataFrame of different params for each of our test images\n",
    "import pandas as pd\n",
    "\n",
    "pred_df = pd.DataFrame({\"img_path\":filepaths,\n",
    "                        \"y_true\":y_labels,\n",
    "                        \"y_pred\":pred_classes,\n",
    "                        \"pred_conf\":preds_probs.max(axis=1), # get max pred prob\n",
    "                        \"y_true_classname\":[class_names[i] for i in y_labels],\n",
    "                        \"y_pred_classname\":[class_names[i] for i in pred_classes]})\n",
    "pred_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Find out which predictions are wrong\n",
    "pred_df[\"pred_correct\"] = pred_df[\"y_true\"] == pred_df[\"y_pred\"]\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Sort our DataFrame to have most wrong predictions at the top\n",
    "top_100_wrong = pred_df[pred_df[\"pred_correct\"]==False].sort_values(\"pred_conf\", ascending=False).head(100)\n",
    "top_100_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Visualize the test data samples which have the wrong prediction but highest prediction probability\n",
    "images_to_view = 9\n",
    "start_index = 20\n",
    "plt.figure(figsize=(15,10))\n",
    "for i, row in enumerate(top_100_wrong[start_index:start_index+images_to_view].itertuples()):\n",
    "    print(i)\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    expanded_img = load_and_prep_images(row[1], scale=False)/255.\n",
    "    plt.imshow(expanded_img)\n",
    "    plt.title(f\"Actual: {row.y_true_classname} \\nPred: {row.y_pred_classname} \\nProb: {row.pred_conf:.2f}\", color=\"r\")\n",
    "    plt.axis(False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out on custom images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get custom images\n",
    "!wget -nc -P ../Downloads/ https://storage.googleapis.com/ztm_tf_course/food_vision/custom_food_images.zip\n",
    "\n",
    "# Unzip\n",
    "unzip_data('../Downloads/custom_food_images.zip', '../Downloads')\n",
    "\n",
    "# Check number of images and subdirectories in the dataset\n",
    "walk_through_dir('../Downloads/custom_food_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the images filepaths\n",
    "custom_food_images = ['../Downloads/custom_food_images/' + img_path for img_path in os.listdir(\"../Downloads/custom_food_images\")]\n",
    "custom_food_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on custom food images\n",
    "for img in custom_food_images:\n",
    "    img = load_and_prep_images(img, scale=False) # load in target image and turn it into tensor\n",
    "    pred_prob = model.predict(tf.expand_dims(img, axis=0)) # make prediction on image with shape [None, 224, 224, 3]\n",
    "    pred_class = class_names[pred_prob.argmax()] # find the predicted class label\n",
    "    # Plot the image with appropriate annotations\n",
    "    plt.figure()\n",
    "    plt.imshow(img/255.) # imshow() requires float inputs to be normalized\n",
    "    plt.title(f\"pred: {pred_class}, prob: {pred_prob.max():.2f}\")\n",
    "    plt.axis(False)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('directml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c179f5f00e546169410edbb1f0aaf45df98c7fd11ac06763c585ffbea32bb02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
