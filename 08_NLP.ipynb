{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is NLP?\n",
    "\n",
    "Natural Language Processing: The ability of a computer program to understand human language as it is spoken and written.\n",
    "\n",
    "Example: OpenAI GPT-3\n",
    "\n",
    "# What is NLU?\n",
    "Natural Language Understanding\n",
    "\n",
    "Difference?\n",
    "\n",
    "Please crack the car window, it is getting hot.\n",
    "\n",
    "nlp will literally crack the window.\n",
    "nlu will understand to slightly open the window.\n",
    "\n",
    "# Sequence Problems\n",
    "1. one to one\n",
    "2. one to many\n",
    "3. many to one\n",
    "4. many to many\n",
    "5. many to many synchronized\n",
    "\n",
    "Use Cases:\n",
    "* Classification\n",
    "* Machine Translation\n",
    "* Text Generation\n",
    "* Voice Assistants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an RNN\n",
    "\n",
    "Recurrent Neural Network (RNN) is a type of neural network that is capable of learning to predict the next element in a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture of an RNN\n",
    "\n",
    "Typical architecture of an RNN is as follows:\n",
    "1. Input Layer\n",
    "2. Text vectorization layer\n",
    "3. Embedding layer\n",
    "4. RNN Cell: LSTM layer\n",
    "    * Tanh activation function\n",
    "5. Hidden Activation layer\n",
    "6. Output Layer\n",
    "    * Sigmoid\n",
    "7. Creation of Model\n",
    "8. Compile\n",
    "9. Fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "from _helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys, walk_through_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a text dataset\n",
    "\n",
    "The dataset we're going to be using is Kaggle's introduction to NLP dataset (text samples of Tweets labelled as disaster or not disaster).\n",
    "\n",
    "[Source](https://www.kaggle.com/c/nlp-getting-started)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc -P ../Downloads/ https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip\n",
    "unzip_data('../Downloads/nlp_getting_started.zip', '../Downloads/08_NLP')\n",
    "\n",
    "# Walkthrough dir\n",
    "walk_through_dir('../Downloads/08_NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing a text dataset\n",
    "\n",
    "To visualize text samples, we need to read them in.\n",
    "\n",
    "Python read-write\n",
    "Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('../Downloads/08_NLP/train.csv')\n",
    "test_df = pd.read_csv('../Downloads/08_NLP/test.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle training dataframe\n",
    "train_df_shuffled = train_df.sample(frac=1, random_state=42)\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check test dataframe\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many examples of each class?\n",
    "train_df.target.value_counts()\n",
    "# 0 = not disaster, 1 = disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many total samples?\n",
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize random training examples\n",
    "import random\n",
    "\n",
    "random_index = random.randint(0, len(train_df) - 5)\n",
    "for row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index + 5].itertuples():\n",
    "    _, text, target = row\n",
    "    print(f\"Target: {target}\", \"(real disaster)\" if target == 1 else \"(not disaster)\")\n",
    "    print(f\"Text:\\n{text}\\n\")\n",
    "    print(\"---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
    "                                                                            train_df_shuffled[\"target\"].to_numpy(),\n",
    "                                                                            test_size=0.1, \n",
    "                                                                            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check lengths\n",
    "\n",
    "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check first 10\n",
    "train_sentences[:10], train_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting text into numbers\n",
    "\n",
    "Tokenization vs Embedding\n",
    "\n",
    "In NLP, there are two main concepts for turning text into numbers:\n",
    "* **Tokenization** - A straight mapping from word or character or sub-word to a numerical value. There are three main levels of tokenization:\n",
    "  1. Using **word-level tokenization** with the sentence \"I love TensorFlow\" might result in \"I\" being `0`, \"love\" being `1` and \"TensorFlow\" being `2`. In this case, every word in a sequence considered a single **token**.\n",
    "  2. **Character-level tokenization**, such as converting the letters A-Z to values `1-26`. In this case, every character in a sequence considered a single **token**.\n",
    "  3. **Sub-word tokenization** is in between word-level and character-level tokenization. It involves breaking invidual words into smaller parts and then converting those smaller parts into numbers. For example, \"my favourite food is pineapple pizza\" might become \"my, fav, avour, rite, fo, oo, od, is, pin, ine, app, le, piz, za\". After doing this, these sub-words would then be mapped to a numerical value. In this case, every word could be considered multiple **tokens**.\n",
    "* **Embeddings** - An embedding is a representation of natural language which can be learned. Representation comes in the form of a **feature vector**. For example, the word \"dance\" could be represented by the 5-dimensional vector `[-0.8547, 0.4559, -0.3332, 0.9877, 0.1112]`. It's important to note here, the size of the feature vector is tuneable. There are two ways to use embeddings: \n",
    "  1. **Create your own embedding** - Once your text has been turned into numbers (required for an embedding), you can put them through an embedding layer (such as [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)) and an embedding representation will be learned during model training.\n",
    "  2. **Reuse a pre-learned embedding** - Many pre-trained embeddings exist online. These pre-trained embeddings have often been learned on large corpuses of text (such as all of Wikipedia) and thus have a good underlying representation of natural language. You can use a pre-trained embedding to initialize your model and fine-tune it to your own specific task.\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-tokenization-vs-embedding.png)\n",
    "*Example of **tokenization** (straight mapping from word to number) and **embedding** (richer representation of relationships between tokens).*\n",
    "\n",
    "It depends on your problem. You could try character-level tokenization/embeddings and word-level tokenization/embeddings and see which perform best. You might even want to try stacking them (e.g. combining the outputs of your embedding layers using [`tf.keras.layers.concatenate`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/concatenate)). \n",
    "\n",
    "If you're looking for pre-trained word embeddings, [Word2vec embeddings](http://jalammar.github.io/illustrated-word2vec/), [GloVe embeddings](https://nlp.stanford.edu/projects/glove/) and many of the options available on [TensorFlow Hub](https://tfhub.dev/s?module-type=text-embedding) are great places to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "\n",
    "sent_lens = [len(sentence.split()) for sentence in train_sentences] \n",
    " \n",
    "# Taking the 95% length as max length\n",
    "max_len = int(np.percentile(sent_lens, 95))\n",
    "\n",
    "print('max len:', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the avg number of tokens (words) in the training tweets\n",
    "\n",
    "round(sum([len(i.split()) for i in train_sentences]) / len(train_sentences))\n",
    "\n",
    "# Setup text vectorization variables\n",
    "max_vocab_length = 10000 # max number of words to have in our vocab\n",
    "\n",
    "text_vectorizer = TextVectorization(\n",
    "                    max_tokens=max_vocab_length, # how many words in the vector\n",
    "                    standardize=\"lower_and_strip_punctuation\", # standardize text\n",
    "                    split=\"whitespace\", # split text into words via whitespace\n",
    "                    ngrams=None, # create groups of n-words\n",
    "                    output_mode=\"int\", # how to map tokens to numbers\n",
    "                    output_sequence_length=max_len, # how long the output sequence should be\n",
    "                    pad_to_max_tokens=True, # pad the output sequence to the max length\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the text vectorizer to the training text       \n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample sentence and tokenize it\n",
    "\n",
    "sample_sentence = \"There's a flood in my street!\"\n",
    "text_vectorizer([sample_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a random setnence from the training dataset and tokenize it\n",
    "random_index = random.randint(0, len(train_sentences) - 1)\n",
    "random_sentence = train_sentences[random_index]\n",
    "vectorized_sentence = text_vectorizer([random_sentence])\n",
    "print(f'original sentence: {random_sentence}\\nVectorized: {vectorized_sentence[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique words in the vocab\n",
    "\n",
    "words_in_vocab = text_vectorizer.get_vocabulary() # get all of the unique words in our training data\n",
    "top_5_words = words_in_vocab[:5] # get the top 5 words (most common)\n",
    "bottom_5_words = words_in_vocab[-5:] # get the bottom 5 words (least common)\n",
    "\n",
    "print(f'Number of words in vocab: {len(words_in_vocab)}\\nTop 5 words: {top_5_words}\\nBottom 5 words: {bottom_5_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Embedding using an Embedding Layer\n",
    "\n",
    "1. `input_dim` = the size of our vocab\n",
    "2. `output_dim` = the size of our output embedding vector\n",
    "3. `input_length` = the length of our input sequence\n",
    "4. `mask_zero` = whether or not to mask zero values in our input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(\n",
    "                            input_dim=max_vocab_length, #set input shape\n",
    "                            output_dim=128, #set output shape\n",
    "                            input_length=max_len #set input length\n",
    "                            )\n",
    "embedding                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random sentence from the training set\n",
    "random_sentence = random.choice(train_sentences)\n",
    "\n",
    "print(f'original sentence: {random_sentence}')\n",
    "\n",
    "# Embed the random sentence (turn it into dense vectors)\n",
    "\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out a single token's embedding\n",
    "sample_embed[0][0], sample_embed[0][0].shape, random_sentence[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling a text dataset\n",
    "\n",
    "Running a series of experiments\n",
    "\n",
    "* **Model 0**: Naive Bayes (baseline)\n",
    "* **Model 1**: Feed-forward neural network (dense model)\n",
    "* **Model 2**: LSTM model\n",
    "* **Model 3**: GRU model\n",
    "* **Model 4**: Bidirectional-LSTM model\n",
    "* **Model 5**: 1D Convolutional Neural Network\n",
    "* **Model 6**: TensorFlow Hub Pretrained Feature Extractor\n",
    "* **Model 7**: Same as model 6 with 10% of training data\n",
    "\n",
    "Each experiment will go through the following steps:\n",
    "* Construct the model\n",
    "* Train the model\n",
    "* Make predictions with the model\n",
    "* Track prediction evaluation metrics for later comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0: Getting a baseline\n",
    "\n",
    "Sklearn's Multinomial Naive Bayes using the TF-IDF formula to convert our words into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create tokenization and modelling pipeline\n",
    "model_0 = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n",
    "    (\"clf\", MultinomialNB()), # model the text using a naive bayes classifier\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate our baseline model\n",
    "baseline_score =  model_0.score(val_sentences, val_labels)\n",
    "print(f'Baseline accuracy score: {baseline_score * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_preds[:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('directml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c179f5f00e546169410edbb1f0aaf45df98c7fd11ac06763c585ffbea32bb02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
